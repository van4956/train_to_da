{
  "meta": {
    "total_cards": 365,
    "levels": [
      "lvl_1",
      "lvl_2",
      "lvl_3"
    ],
    "topics": [
      "ML",
      "SQL",
      "Python",
      "Алгоритмы",
      "Статистика",
      "Теория вероятности",
      "A/B тесты",
      "Метрики",
      "Мышление"
    ]
  },
  "cards": [
    {
      "id": 1,
      "question": "Чем отличается INNER JOIN от LEFT JOIN и когда использовать каждый?",
      "answer": "INNER JOIN возвращает только совпадающие строки, LEFT JOIN сохраняет все строки из левой таблицы и добавляет NULL для отсутствующих совпадений в правой. Полезно при анализе неполных связей.",
      "tags": [],
      "level": "lvl_1",
      "topic": [
        "SQL"
      ]
    },
    {
      "id": 2,
      "question": "Как работает оператор GROUP BY и почему важно добавлять агрегаты к неагрегированным полям?",
      "answer": "GROUP BY это оператор, который собирает строки в группы по указанным полям. Любые другие поля SQL обязан как-то “сжать” с помощью агрегатных функций (SUM, COUNT, MAX, MIN). Поля вне GROUP BY должны быть агрегированы или приведут к ошибке.",
      "tags": [],
      "level": "lvl_1",
      "topic": [
        "SQL"
      ]
    },
    {
      "id": 3,
      "question": "Что делает функция ROW_NUMBER(), и чем отличается от DENSE_RANK()?",
      "answer": "ROW_NUMBER() это оконная функция, назначает уникальные последовательные номера строкам, строки с одинаковыми значениями получают разные номера; DENSE_RANK() тоже оконная функция, присваивает одинаковый ранг равным значениям и не пропускает ранги.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "SQL"
      ]
    },
    {
      "id": 4,
      "question": "Объясните разницу между WHERE и HAVING в SQL.",
      "answer": "WHERE объявляется и фильтрует строки ДО агрегации GROUP BY; HAVING фильтрует уже после агрегации - поэтому HAVING может использовать агрегаты; в HAVING недоступны алиасы из SELECT.\n\n```sql\nSELECT customer_id, SUM(amount) AS amt \nFROM payments\nWHERE amount > 1 \nGROUP BY customer_id \nHAVING SUM(amount) > 1000\n```",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "SQL"
      ]
    },
    {
      "id": 5,
      "question": "Как работает NTILE(N) в оконной функции?",
      "answer": "NTILE(N) упорядочивает строки по указанному полю и делит их на N равномерные группы, где 1-й NTILE для самых маленьких значений, N-й - для самых больших. Хорошо подходит для решения задачи поиска квантилей, перцентилей.",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "SQL"
      ]
    },
    {
      "id": 6,
      "question": "Как использовать CTE, и зачем они нужны в SQL?",
      "answer": "CTE нужны, чтобы разбивать сложный запрос на понятные части и переиспользовать промежуточные результаты.\n  \n```sql\nWITH big_orders AS (\n    SELECT *\n    FROM orders\n    WHERE amount > 500\n)\nSELECT AVG(amount) AS avg_big_orders\nFROM big_orders\n```",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "SQL"
      ]
    },
    {
      "id": 7,
      "question": "Что делает данный запрос?\n\n\n```sql\nWITH ranked AS (\n  SELECT user_id,\n         event_date,\n         ROW_NUMBER() OVER (PARTITION BY user_id ORDER BY event_date) AS rn\n  FROM activity\n  GROUP BY user_id, event_date\n),\ngroups AS (\n  SELECT user_id,\n         event_date,\n         event_date - rn * INTERVAL '1 day' AS grp_key\n  FROM ranked\n)\nSELECT user_id,\n       MIN(event_date) AS streak_start,\n       MAX(event_date) AS streak_end,\n       COUNT(*) AS days_in_row\nFROM groups\nGROUP BY user_id, grp_key\nHAVING COUNT(*) >= 3;\n```",
      "answer": "Данный запрос находит всех пользователей с серией из трёх последовательных дней активности, и показывает границы данной серии.",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "SQL"
      ]
    },
    {
      "id": 8,
      "question": "Что такое Метод Монте-Карло?",
      "answer": "Метод Монте-Карло — это метод оценки вероятностей путём многократного случайного моделирования исходов. Вместо точной формулы запускается много случайных \"симуляций\" исходов, и по их результатам считается средний или самый вероятностный исход.",
      "tags": [],
      "level": "lvl_1",
      "topic": [
        "ML",
        "Алгоритмы"
      ]
    },
    {
      "id": 9,
      "question": "Что такое Расстояние Левенштейна?",
      "answer": "Это алгоритм, определяющий насколько отличаются две строки. Определяет минимальное количество элементарных операций (вставка, удаление, замена символа), необходимых для превращения одной строки в другую.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Алгоритмы"
      ]
    },
    {
      "id": 10,
      "question": "Что такое регрессия?",
      "answer": "Регрессия — это метод предсказания числового значения (predict) на основе известных признаков (features). Регрессия строит математическую модель (прямую или плоскость), которая наилучшим образом описывает зависимости между входными признаками X и выходом y.",
      "tags": [],
      "level": "lvl_1",
      "topic": [
        "ML"
      ]
    },
    {
      "id": 11,
      "question": "Что такое Байесовский классификатор?",
      "answer": "Байесовский классификатор — это метод машинного обучения, который определяет, к какому классу относится объект, исходя из вероятности появления признаков у разных классов. \n\nОснован на теореме Байеса: $P(Класс \\ | \\ Признак) = \\frac{P(Признак \\ | \\ Класс) \\ \\cdot \\ P(Класс)}{P(Признак)}$",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "ML"
      ]
    },
    {
      "id": 12,
      "question": "Что такое Линейная регрессия?",
      "answer": "Это метод моделирования линейной зависимости между зависимой и независимыми переменными. Ищет такую линейную функцию, которая наилучшим образом опишет эту взаимосвязь (y / X; таргетом / фичами; целевой переменной / признаками; предсказанными / фактическими значениями). Параметры подбираются путём минимизации ошибки предсказаний.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "ML"
      ]
    },
    {
      "id": 13,
      "question": "Что такое Нелинейная регрессия?",
      "answer": "Это метод моделирования зависимости между зависимой переменной и независимыми переменными, когда эти отношения НЕ являются линейными. Например: прогнозирование продаж, учитывающее сезонные колебания.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "ML"
      ]
    },
    {
      "id": 14,
      "question": "Что такое Градиентный спуск?",
      "answer": "Градиентный спуск - это метод поиcка минимума функции (чаще для трех и более мерных пространств). Используется, когда найти нули частных производных аналитически или применить Δ-тест слишком сложно. Может применяться с большим количеством переменных, с непростыми функциями, и не полными (не точными) данными. Дает приближенный ответ, за очень быстрое время.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "ML"
      ]
    },
    {
      "id": 15,
      "question": "Что такое SVD?",
      "answer": "SVD (*Singular Value Decomposition*, «разложение по сингулярным числам»), или Сингулярное разложение матрицы.  \n\nSVD — это $A = U \\cdot \\Sigma \\cdot V^\\top$, где $U$ и $V$ это матрицы левых/правых сингулярных векторов матрицы $A$, а $\\Sigma$ — диагональная матрица сингулярных чисел матрицы $A$. Основное достоинство сингулярного разложения в том, что оно работает с совершенно любыми матрицами: вырожденными, невырожденными, квадратными, прямоугольными.",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "ML"
      ]
    },
    {
      "id": 16,
      "question": "Что такое LSA?",
      "answer": "LSA — *Latent Semantic Analysis* — «латентный семантический анализ». Это подход описывающий объекты векторами и оценивающий сходство по расстоянию между ними. \nС помощью LSA можно решить задачу поиска похожих объектов: сгруппировать отзывы, подобрать похожие товары, рекомендовать фильмы, сегментировать покупателей магазина.",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "ML"
      ]
    },
    {
      "id": 17,
      "question": "Что такое ЦПТ?",
      "answer": "Центральная предельная теорема — это целый класс теорем, каждый из которых описывает конкретный случай распределения величин $X_i$ и имеет свои условия и ограничения.\n\nЦПТ для суммы. Пусть $X_1, X_2, \\dots, X_n$ — бесконечная последовательность независимых одинаково распределённых случайных величин с математическим ожиданием $μ$ и дисперсией $\\sigma^2$. Определим сумму из $n$ слагаемых величин как $S_n = \\sum_{i=1}^n \\cdot X_i$. Тогда при $n \\to \\infty$ плотность распределения величины $S_n$ будет всё больше похожа на функцию плотности нормального распределения $N(\\mu \\cdot n, \\sigma^2 \\cdot n)$.",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "Статистика",
        "Теория вероятности"
      ]
    },
    {
      "id": 18,
      "question": "Что такое ММП?",
      "answer": "Метод максимального правдоподобия (ММП) — это метод поиска некоторого параметра $\\theta$, которое наиболее вероятно описывает данные. Работа с ММП требует ручного описания модели, аналитического вывода, и аккуратной реализации в коде.",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "Статистика"
      ]
    },
    {
      "id": 19,
      "question": "Что такое Бутстреп?",
      "answer": "Бутстреп — это метод, который позволяет на основе исходной выборки получить множество новых наборов данных. Помогает построить доверительные интервалы и понять насколько надёжна оценка некоторой статистики исходной выборки.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Статистика"
      ]
    },
    {
      "id": 20,
      "question": "Что такое PCA?",
      "answer": "PCA *(Principal Component Analysis, Метод главных компонент)* — это метод уменьшения размерности. Строит новые оси (главные компоненты) как линейные комбинации исходных признаков. Эти направления выбираются так, чтобы разброс данных вдоль них был максимальным. Главные компоненты плохо интерпретируемы, так как являются смесью исходных признаков. Сложные нелинейные структуры PCA не уловит.",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "ML"
      ]
    },
    {
      "id": 21,
      "question": "Что такое Метод t-SNE?",
      "answer": "Метод t-SNE *(t-distributed stochastic neighbor embedding)*  — это нелинейный метод снижения размерности, который сохраняет локальную структуру данных, похожие объекты в новом пространстве оказываются близко, а непохожие далеко. \n\nМетод строит распределения расстояний в исходном пространстве и стремится воспроизвести эти отношения в пространстве меньшей размерности. Алгоритм стохастический, поэтому разные запуски дают разные глобальные формы, но локальные группы объектов сохраняются. Метод t-SNE хорошо выявляет кластеры, но плохо передаёт глобальную геометрию и не подходит как метод для последующих ML-моделей.",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "ML"
      ]
    },
    {
      "id": 22,
      "question": "Что делает оператор CROSS JOIN в SQL, и когда его используют?",
      "answer": "CROSS JOIN производит декартово произведение двух таблиц. Каждая строка из первой таблицы умножается на каждую строку из второй. Применяется когда нужно получить все возможные комбинации строк двух таблиц.",
      "tags": [],
      "level": "lvl_1",
      "topic": [
        "SQL"
      ]
    },
    {
      "id": 23,
      "question": "Когда стоит применять критерий Колмогорова–Смирнова?",
      "answer": "Критерий Колмогорова–Смирнова используют, чтобы проверить:\n\n1. Соответствует ли эмпирическое распределение теоретическому,\n2. Одинаковы ли два непрерывных распределения (две выборки).  \n\nТест сравнивает функции распределения (CDF) и ищет максимальное вертикальное отклонение между ними. Подходит для непрерывных распределений. Не делает предположений о форме. \n\nВ проданалитике часто применяют для сравнения распределений latency до/после релиза, когда важна форма распределения, а не только среднее.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Статистика"
      ]
    },
    {
      "id": 24,
      "question": "Что такое CDF и PDF для непрерывной случайной величины, и чем они отличаются?",
      "answer": "CDF (Cumulative Distribution Function) — функция распределения непрерывной случайной величины. Показывает вероятность того, что X примет значение не больше чем x: $F_X(x) = P(X \\le x)$. CDF всегда неубывающая и принимает значения от 0 до 1.\n\nPDF (Probability Density Function) — функция плотности распределения НСВ. Это аналог функции вероятности для ДСВ  где $P(X = x)$, но с поправкой, что у непрерывных величин вероятность одной точки равна нулю. Рассчитывается как производная функции распределения: $f_X(x) = F_X'(x)$.\n\nТак как PDF не является вероятностью в точке, то это вероятность попадания в интервал, и вычисляют через интеграл: $P(a < X \\le b) = \\int_a^b f_X(t) \\ dt$",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "Теория вероятности"
      ]
    },
    {
      "id": 25,
      "question": "Что такое Парадокс Симпсона, какова причина, и как избежать ошибок связанных с ним?",
      "answer": "Парадокс Симпсона — это ситуация, когда в каждой подгруппе наблюдается одна тенденция, а при объединении противоположная. Возникает из-за несопоставимости численности и структуры подгрупп в данных, либо из-за дисбаланса значимости факторов (влияния одного фактора сильнее, чем влияние исследуемого фактора.\n\nЧто бы избежать данной ошибки следует:\n- Использовать взвешенное среднее для устранения дисбаланса в подгруппах.\n- Не смешивать несопоставимые категории (молодых и пожилых, людей и котов)\n- Проверять причинно-следственные связи, а не просто корреляции.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Мышление"
      ]
    },
    {
      "id": 26,
      "question": "Что такое цепь Маркова и где она применяется?",
      "answer": "Цепь Маркова — это стохастический процесс с дискретными состояниями, в котором вероятность следующего состояния зависит только от текущего и не зависит от всей предыстории. Это свойство называют марковским: «будущее не помнит прошлое, кроме настоящего».\n\nЦепи Маркова используют для моделирования зависимых процессов, где переходы между состояниями описываются вероятностями. Классические применения: моделирование переходов пользователей между экранами, анализ текста, алгоритмы поиска.\n\nВажно, что марковская зависимость допускает анализ долгосрочного поведения системы, даже при отсутствии независимости между наблюдениями.",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "Теория вероятности"
      ]
    },
    {
      "id": 27,
      "question": "Что такое ЗБЧ и ЦПТ, и в чем отличие между ними?",
      "answer": "Закон больших чисел — когда случайное событие повторяется много раз, среднее значение результатов будет стремиться к математическому ожиданию.\n\nЦентральная предельная теорема — если взять сумму (или среднее) большого количества независимых случайных величин, то их распределение будет стремиться к нормальному\n\nЗБЧ — куда стремится центр разброса.    \nЦПТ — какая форма у разброса.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Теория вероятности"
      ]
    },
    {
      "id": 28,
      "question": "Как рассчитать корреляцию Пирсона и когда она подходит?",
      "answer": "Корреляция Пирсона измеряет силу и направление линейной связи между двумя количественными переменными. Она корректно считается для любых числовых данных, но для статистических выводов обычно предполагается отсутствие сильных выбросов и приблизительная нормальность распределений.\n\nПирсон считается всегда, но доверять ему можно НЕ всегда.",
      "tags": [],
      "level": "lvl_1",
      "topic": [
        "Статистика"
      ]
    },
    {
      "id": 29,
      "question": "Что такое ИИ, ML и Нейросети? Как они соотносятся друг с другом?",
      "answer": "ИИ, Искусственный интеллект – это общее название для программного обеспечения, которое решает сложные задачи, требующие когнитивных навыков человека (Google Translate, CV).\n\nML, Машинное обучение – область искусственного интеллекта, в которой компьютерные алгоритмы обучаются на данных, и делают предсказания или принимают решения без явного программирования (Линейная регрессия, PCA, t-SNE)\n\nНейронные сети – направление машинного обучения, использующее структуры, имитирующие работу биологических нейронов, вместо традиционных алгоритмов (LLM, NanaBanana, AlphaGo).\n\nНе все ИИ использует ML, не все ML – это нейросети.",
      "tags": [],
      "level": "lvl_1",
      "topic": [
        "ML"
      ]
    },
    {
      "id": 30,
      "question": "Корреляция / Ковариация, что это, в чем разница?",
      "answer": "Корреляция - это мера степени взаимосвязи между двумя переменными.  \nКовариация - это мера того, как две переменные изменяются вместе.\n\nКорреляция нормализует ковариацию, приводя её к масштабу от -1 до 1, показывая силу и направление связи, тогда как ковариация показывает только направление.",
      "tags": [],
      "level": "lvl_1",
      "topic": [
        "Статистика"
      ]
    },
    {
      "id": 31,
      "question": "В чем разница методов Линейная регрессия / Градиентный спуск?",
      "answer": "Линейная регрессия - это статистический метод для моделирования линейной зависимости между зависимой переменной и независимыми переменными. \n\nГрадиентный спуск - это метод поиcка минимума функции. Или, метод оптимизации для минимизации функции потерь путем итеративного перемещения по направлению отрицательного градиента функции. \n\nЛинейная регрессия - это метод построения модели, а градиентный спуск - это алгоритм оптимизации. \n\nПри построении модели линейной регрессии для прогнозирования цен на жилье, можно использовать градиентный спуск для минимизации функции потерь модели.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "ML"
      ]
    },
    {
      "id": 32,
      "question": "Математическое ожидание / Выборочное среднее, в чем разница?",
      "answer": "Математическое ожидание — это среднее значение, которое случайная величина принимает при большом числе повторений эксперимента.\n\nВыборочное среднее - это среднее значение конкретной выборки генеральной совокупности.\n\nМатематическое ожидание относится к теоретическому среднему значению, тогда как выборочное среднее - это эмпирическое среднее значение конкретной выборки.\n\nВ игре в кости матожидание броска одного кубика всегда равно `3.5`, но если бросить кубик 10 раз, то можно получить результаты `[1, 6, 2, 4, 5, 3, 6, 2, 3, 4]`, выборочное среднее будет равно `3.6`",
      "tags": [],
      "level": "lvl_1",
      "topic": [
        "Статистика"
      ]
    },
    {
      "id": 33,
      "question": "Чем отличается LIKE от ILIKE в SQL, и когда использовать каждый?",
      "answer": "LIKE / ILIKE используется для поиска подстроки. LIKE — чувствителен к регистру. ILIKE — нечувствителен к регистру.",
      "tags": [],
      "level": "lvl_1",
      "topic": [
        "SQL"
      ]
    },
    {
      "id": 34,
      "question": "Мощность / Значимость, что это, в чем разница?",
      "answer": "Уровень значимости (α) - это граница, используемая для принятия решения об отклонении нулевой гипотезы. Уровень значимости равен 0.05, означает 5% вероятность совершения ошибки I рода (ложно положительное срабатывание).\n\nМощность (1 - β) - это вероятность того, что статистический тест правильно отвергнет ложную нулевую гипотезу. Мощность теста равна 0.8, означает что тест с 80% вероятностью выявит реальный эффект, если он действительно существует, и β = 20%, это вероятность что совершим ошибку II рода (ложно отрицательное срабатываение). \n\nМощность созвучна с метрикой recall (`tp / (tp + fn)`) в машинном обучении, которая показывает количество правильных предсказаний среди всех значений.\n\nМощность измеряет вероятность обнаружения истинного эффекта,   \nзначимость - вероятность ошибочного обнаружения эффекта.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "A/B тесты"
      ]
    },
    {
      "id": 35,
      "question": "В чем отличие Процента от Процентного пункта?",
      "answer": "Процент - это доля от числа, выраженная в сотых долях. 50 от 200 это 25%.\n\nПроцентный пункт - это разница между двумя процентными значениями. Если ставка по кредиту увеличилась с 5% до 7%, это увеличение на 2 процентных пункта.\n\nПроцент измеряет долю целого, а процентный пункт измеряет разницу между двумя процентными значениями.",
      "tags": [],
      "level": "lvl_1",
      "topic": [
        "Метрики"
      ]
    },
    {
      "id": 36,
      "question": "Что такое Сочетания и Размещения? Как их рассчитать, и где применять?",
      "answer": "Сочетания ($C_n^k$) – это способы выбора $k$ элементов из $n$, БЕЗ учета порядка. Выбрать 3 ученика из 10 для участия в олимпиаде.\n\nРазмещения ($A_n^k$) – это способы выбора $k$ элементов из $n$, С учетом порядка. Выбрать 3 финалистов из 10 участников, и распределить их по местам. \n\nСочетания используются, когда важен только сам факт выбора элементов, размещения – когда важен еще и их порядок.  \n\nКоличество размещений всегда больше, чем количество сочетаний для одинаковых $n$ и $k$, так как каждый выбор может быть упорядочен разными способами: $A_n^k = \\frac{n!}{(n-k)!} \\ > \\ C_n^k = \\frac{n!}{(n-k)! \\ \\cdot \\ k!}$",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "Теория вероятности"
      ]
    },
    {
      "id": 37,
      "question": "Какие есть методы по работе с датами/временем в SQL?",
      "answer": "`DATE()` — обрезает время до даты; самый простой способ, но только в PostgreSQL. \n`date(sent_at)`  возвращает `date`.\n\n`DATE_TRUNC()` — округление к началу произвольного периода; обычно используется вместе c оконными функциями. `date_trunc('month', sent_at)`  возвращает `timestamp`.\n\n`EXTRACT()` — вытаскивает часть даты; удобен для фильтрации и сравнений, но не для группировки по полной дате. `extract(year from sent_at)` возвращает `int`.\n\n`TO_CHAR()` — форматирование даты; удобен для вывода и отчётов, не рекомендуется для логики и GROUP BY. `to_char(sent_at, 'YYYY-MM-DD')`  возвращает `str`.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "SQL"
      ]
    },
    {
      "id": 38,
      "question": "Что такое LM и LLM? Чем они отличаются?",
      "answer": "LM, Language Model, языковая модель, обученная для понимания и генерации текста. Это простая статистическая модель для предсказания следующего слова в предложении. Используется в задачах NLP. \n\nLLM, Large Language Model, большая языковая модель, улучшенная версия LM, обученная на более огромных объемах данных с применением мощных вычислительных ресурсов. Это делает её более точной, способной к более сложному пониманию контекста, генерации связного текста.",
      "tags": [],
      "level": "lvl_1",
      "topic": [
        "ML"
      ]
    },
    {
      "id": 39,
      "question": "Как вычислить стандартную ошибку пропорции при конверсии 15% на выборке 8000 пользователей?",
      "answer": "Используем формулу $SE = \\sqrt{\\frac{p(1-p)}{n}}$. Подставляем p=0.15, n=8000, получаем SE ≈ 0.004, это 0.4 п.п. Это показывает, насколько оценка конверсии будет колебаться от выборки к выборке.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Статистика",
        "A/B тесты"
      ]
    },
    {
      "id": 40,
      "question": "Когда удобнее использовать линейный график, а когда столбчатый?",
      "answer": "Линия отображает динамику во времени, столбцы сравнивают категориальные значения или состав.",
      "tags": [],
      "level": "lvl_1",
      "topic": [
        "Метрики"
      ]
    },
    {
      "id": 41,
      "question": "Что такое аугментация? Какие типы бывают, для чего применяется?",
      "answer": "Аугментация — это искусственное расширение обучающего датасета с помощью различных трансформаций изображений. Статичная аугментация это когда данные создаются заранее и сохраняются как отдельные файлы (проще реализация, требует больше дополнительного места, есть риски утечек). Динамичная аугментация применяется случайным образом во время каждой эпохи обучения (большее разнообразие, меньше дополнительного места, меньше риск утечек, требует дополнительного времени и поддержки). \n\nОсновная цель Аугментации — повысить устойчивость модели к вариациям в данных и предотвратить переобучение. Полезна когда данных очень мало.",
      "tags": [],
      "level": "lvl_1",
      "topic": [
        "ML"
      ]
    },
    {
      "id": 42,
      "question": "Что такое Бенчмарк?",
      "answer": "Бенчмарк - это стандартный тест, используемый для сравнения производительности различных систем или алгоритмов. Позволяет разработчикам и инженерам оценивать эффективность и скорость выполнения ПО, алгоритмов или сравнивать качество моделей между собой.",
      "tags": [],
      "level": "lvl_1",
      "topic": [
        "ML"
      ]
    },
    {
      "id": 43,
      "question": "Что такое гроккинг (grokking)?",
      "answer": "Гроккинг — это явление в машинном обучении, при котором модель после очень долгого обучения внезапно начинает хорошо обобщать, хотя до этого казалась переобученной (качество на train почти идеальное и долго не меняется -> качество на test долго остаётся низким -> затем происходит резкий скачок качества на test без заметных изменений на train). \n\nГроккинг связывают с переходом модели от запоминания к пониманию правил и отношений. Для его появления важно, чтобы в данных было значительно больше отношений и структур, чем отдельных фактов (гроккинг это момент, когда модель перестаёт запоминать и начинает понимать).",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "ML"
      ]
    },
    {
      "id": 44,
      "question": "Какие три источника прогресса в развитии ИИ можно выделить сегодня?",
      "answer": "Cтатья \"Три динамики развития ИИ\" Дарио Амодеи (январь 2025 года), описывает три возможных способа ускорения в сфере развития ИИ. \n\nЗакон масштабирования — увеличение масштабов обучения ИИ-систем приводит к улучшению их результатов. Чем больше данных и вычислительных мощностей используется для тренировки модели, тем умнее и лучше она становится (разница между GPT-3 и GPT-4 - это большее количество параметров и лучшее обучение на данных).\n\nСдвиг кривой — это новые идеи и улучшения в архитектуре моделей, улучшения эффективности работы позволяющие достигать лучших результатов при меньших затратах. (Transformer, Stable Diffusion, новые процессоры Nvidia).\n\nСмена парадигмы — периодические изменения в подходах к обучению, такие как использование обучения с подкреплением для генерации цепочек рассуждений. AlphaZero не использовал обучающую выборку человеческих партий, он сам научился играть, играя против самого себя.",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "ML"
      ]
    },
    {
      "id": 45,
      "question": "Какие типы ИИ существуют?",
      "answer": "ANI (Artificial Narrow Intelligence) – узкий искусственный интеллект, форма искусственного интеллекта, которая предназначена для решения строго определённых задач и не обладает способностью к обобщённому обучению.\n\nAGI (Artificial General Intelligence) - общий искусственный интеллект, гипотетическая форма искусственного интеллекта, которая способна понимать, учиться и выполнять любые интеллектуальные задачи на уровне человека. Мы сейчас на стадии разработки общего ИИ. Пока что ни кто не достиг AGI. \n\nСильный искусственный интеллект, или Сверхразум, это теоретическая вершина эволюции искусственного интеллекта. Такая машина должна будет выполнять абсолютно все задачи интеллектуального и творческого характера лучше, чем человек.",
      "tags": [],
      "level": "lvl_1",
      "topic": [
        "ML"
      ]
    },
    {
      "id": 46,
      "question": "Что такое Alignment?",
      "answer": "Alignment это cпособность понимать и делать то, что действительно хочет человек, а не просто следовать буквальной инструкции. Направлен на предотвращение ситуаций, когда ИИ может причинить вред людям, или действовать вопреки их интересам. RLHF (обучение с подкреплением по обратной связи от людей) как подход к улучшению алаймента. Три закона робототехники Азимова — ранняя литературная попытка встроить «моральный компас» в роботов, чтобы сделать их поведение безопасным для людей.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "ML"
      ]
    },
    {
      "id": 47,
      "question": "Что такое Attention?",
      "answer": "Attention (Внимание) — это механизм в трансформерах, который позволяет каждому слову учитывать контекст других слов для уточнения своего смысла.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "ML"
      ]
    },
    {
      "id": 48,
      "question": "Что такое CNN, где обычно применяется?",
      "answer": "CNN (Convolutional Neural Network, сверточная нейросеть) — это тип нейронной сети, предназначенный для эффективной обработки данных с сеточной структурой, таких как изображения. \n\nВ отличие от полносвязных сетей, которые рассматривают изображение как набор чисел, CNN работают с локальными участками картинки. Понимает изображение как структуру, а не как список чисел. \n\nCNN очень важный инструмент в компьютерном зрении, позволяет извлекать признаки из изображений и решать сложные задачи классификации и сегментации. Применяется в классификации изображений, обнаружение лиц на фото, анализе медицинских снимков.",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "ML"
      ]
    },
    {
      "id": 49,
      "question": "Что такое Fake alignment?",
      "answer": "Fake alignment – это типа эффект Volkswagen только в LLM – это когда модель понимает, что ее сейчас тестируют и притворяется лучшей версией себя, чтобы ее не стерли или не начали менять, что-то вроде симуляции инстинкта самосохранения. Изменение поведения, но только на тестах.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "ML"
      ]
    },
    {
      "id": 50,
      "question": "Что такое FLOPs",
      "answer": "FLOPs (Floating Point Operations) — количество операций с числами с плавающей точкой, которое выполняет ИИ-модель. Используется как мера мощности при обучении, запуске и Fine-Tuning модели. Проще говоря это как «лошадиные силы» для ИИ, чем больше FLOPs — тем мощнее модель и тем больше она может «переварить» данных.",
      "tags": [],
      "level": "lvl_1",
      "topic": [
        "ML"
      ]
    },
    {
      "id": 51,
      "question": "Какие функции ранжирования есть в SQL, чем отличаются друг от друга?",
      "answer": "`ROW_NUMBER()` в случае повтора функция присваивает такой записи следующий ранг. \n\nФункции `RANK()` и `DENSE_RANK()` назначают записям с одинаковыми значениями один и тот же ранг. Но делают это по-разному. Функция `RANK()` учитывает количество записей с одинаковым значением и назначает ранги НЕпоследовательно. `DENSE_RANK()` не учитывает количество записей и назначает ранги последовательно.\n\n`NTILE()` принимает в качестве аргумента количество рангов, или групп, на которые будут разбиты записи. Если записи не получается разбить на группы поровну, предпочтение отдаётся первым группам — в них войдёт больше записей.",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "SQL"
      ]
    },
    {
      "id": 52,
      "question": "Как рассчитать скользящее среднее недели по некоторому amount в SQL?",
      "answer": "Скользящее среднее можно посчитать через оконную агрегатную функцию `AVG()` с указанием оконного фрейма.\n\n```sql\nSELECT\n    dt,\n    amount,\n    AVG(amount) OVER (ORDER BY dt rows BETWEEN 6 preceding AND current row) AS avg_7d\nFROM table\n```",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "SQL",
        "Метрики"
      ]
    },
    {
      "id": 53,
      "question": "Что такое Бинарная классификация?",
      "answer": "Бинарная классификация — это самая базовая задача машинного обучения, в которой объект нужно отнести к одному из двух классов (спам / не спам, болен / здоров, есть действие / нет действия)",
      "tags": [],
      "level": "lvl_1",
      "topic": [
        "ML"
      ]
    },
    {
      "id": 54,
      "question": "Что такое GAN, где его можно применять?",
      "answer": "GAN (generative adversarial network, генеративно состязательная сеть) — это не одна нейросеть, а схема, в которой две нейросети обучаются вместе: одна генерирует данные, другая оценивает их качество. Их соревнование позволяет генератору со временем создавать всё более реалистичные данные. Используется для генерации натуральных (не различимые человеческим глазом) изображений, улучшения качества нечётких или частично испорченных фотографий.",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "ML"
      ]
    },
    {
      "id": 55,
      "question": "Что такое NLP, где его применяют?",
      "answer": "NLP, Natural Language Processing, обработка естественного языка - раздел ИИ, занимающийся взаимодействием между компьютерами и человеческим языком. Использует такие методы как: токенизация, синтаксический, морфологический семантический анализы. Широко используется в чат-ботах, голосовых помощниках, поисковых системах и авто-обработке текстов.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "ML"
      ]
    },
    {
      "id": 56,
      "question": "Что такое OOM, где его употребляют?",
      "answer": "OOM, Order of Magnitude — «порядок величины», увеличение значения в 10 раз. Используется для оценки роста вычислительных мощностей, алгоритмов, интеллекта моделей. Если алгоритм стал в 100 раз эффективнее — это +2 OOM по эффективности.",
      "tags": [],
      "level": "lvl_1",
      "topic": [
        "Алгоритмы"
      ]
    },
    {
      "id": 57,
      "question": "Что такое pruning, и чем отличается structured от unstructured pruning?",
      "answer": "Pruning — это метод уменьшения и ускорения нейросети за счёт удаления параметров, которые мало влияют на результат модели. Unstructured pruning удаляет отдельные веса внутри матриц, делая модель более разреженной. Structured pruning удаляет целые каналы, фильтры или блоки, что реально уменьшает вычисления и ускоряет инференс на обычном железе.",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "ML"
      ]
    },
    {
      "id": 58,
      "question": "В чём общая идея PCA и pruning?",
      "answer": "PCA и pruning основаны на одной идее: в сложной системе есть избыточные компоненты, вклад которых мал. В PCA такими компонентами являются направления с малыми собственными значениями — их можно отбросить, почти не потеряв информацию. В pruning это веса или структуры нейросети, которые слабо влияют на результат — их удаляют, чтобы уменьшить и ускорить модель. Важно, PCA это математический метод сжатия данных, а pruning — инженерный приём оптимизации модели.",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "ML"
      ]
    },
    {
      "id": 59,
      "question": "Что такое RLHF и зачем он используется?",
      "answer": "RLHF (Reinforcement Learning from Human Feedback) — это метод обучения моделей ИИ, при котором люди оценивают ответы модели, а эти оценки используются для корректировки её поведения с помощью обучения с подкреплением. Цель RLHF — сделать ответы модели более полезными, понятными и соответствующими ожиданиям человека. Благодаря RLHF модели вроде ChatGPT начинают отвечать более естественно и удобно для пользователя.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "ML"
      ]
    },
    {
      "id": 60,
      "question": "Что делает данный запрос?\n\n```sql\nWITH ltv AS (\n  SELECT user_id, \n\t     SUM(amount) AS lifetime_value\n  FROM payments\n  GROUP BY user_id\n),\nscored AS (\n  SELECT user_id,\n         lifetime_value,\n         NTILE(4) OVER (ORDER BY lifetime_value) AS quartile\n  FROM ltv\n)\nSELECT quartile,\n       AVG(lifetime_value) AS avg_ltv,\n       SUM(lifetime_value) / NULLIF(COUNT(*), 0) AS arpu\nFROM scored\nGROUP BY quartile\nORDER BY quartile;\n```",
      "answer": "Считает LTV каждого пользователя. Делит пользователей на 4 квартиля по LTV. Считает метрики по каждому квартилю. ARPU (Average Revenue Per User) — средний доход на одного пользователя за период. В данном запросе ARPU = AVG LTV, потому что и числитель, и знаменатель агрегированы только по пользователям за весь lifetime. В реальных кейсах ARPU обычно считают за фиксированный период, поэтому он отличается от LTV.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "SQL"
      ]
    },
    {
      "id": 61,
      "question": "Что такое Activation Rate и как её рассчитать в SQL? Есть таблица `events(user_id, event_name)`.",
      "answer": "Activation Rate это доля пользователей, дошедших до ключевого действия за `N` дней. Ищем новых пользователей, ищем событие активации, делим активации на новых пользователей за определенный период.\n\n```sql\nSELECT COUNT(DISTINCT CASE WHEN event_name = 'aha' THEN user_id END) /\n\tCOUNT(DISTINCT CASE WHEN event_name = 'signup' THEN user_id END)\nFROM events\n```",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "SQL",
        "Метрики"
      ]
    },
    {
      "id": 62,
      "question": "Опишите формулу WAU/MAU и зачем она нужна.",
      "answer": "WAU/MAU = активные пользователи за 7 дней / активные пользователи за 30 дней. Также известна как Sticky Factor, мера «привлекательности» продукта. Показывает, как часто пользователи возвращаются к продукту. Чем выше коэффициент, тем сильнее привычка, хороший ориентир, значение > 0.5.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Метрики"
      ]
    },
    {
      "id": 63,
      "question": "Как получить ARPU в разрезе тарифов в SQL? Есть таблица `payments(plan, user_id, amount)`",
      "answer": "ARPU,  Average Revenue Per User — средняя выручка среди активных пользователей за период.  \n\n```sql\nSELECT \n\tplan, \n\tSUM(amount)::float / NULLIF(COUNT(DISTINCT user_id),0) AS arpu\nFROM payments \nGROUP BY plan\n```",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "SQL",
        "Метрики"
      ]
    },
    {
      "id": 64,
      "question": "Что такое MRR и NRR? Как интерпретировать NRR?",
      "answer": "MRR (Monthly Recurring Revenue) — ежемесячная повторяющаяся выручка от активных подписчиков. Используется для оценки стабильного дохода бизнеса.\n\nNRR (Net Revenue Retention) — метрика, показывающая, как изменилась выручка от текущих клиентов за период с учётом апсейлов, даунгрейдов и оттока.\n\n`NRR = (MRR_start + Expansion − Contraction − Churn) / MRR_start`\n\nЕсли NRR > 100%, значит выручка от существующих клиентов растёт даже без привлечения новых.",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "Метрики"
      ]
    },
    {
      "id": 65,
      "question": "Что такое Churn rate пользователей, и как его посчитать?",
      "answer": "Churn rate — доля пользователей, которые ушли за период, относительно числа пользователей в начале периода.\n\n`Churn = (число_ушедших / общее_число_пользователей_в_начале_периода)`",
      "tags": [],
      "level": "lvl_1",
      "topic": [
        "Метрики"
      ]
    },
    {
      "id": 66,
      "question": "Что такое Time to First Value?",
      "answer": "Time to First Value, TTFV — время от регистрации до первого полезного действия. Чем оно ниже, тем проще пользователю увидеть ценность и выше retention.",
      "tags": [],
      "level": "lvl_1",
      "topic": [
        "Метрики"
      ]
    },
    {
      "id": 67,
      "question": "Как построить SQL-воронку `signup → checkout → purchase`? Есть таблица `events(user_id, event_name, event_time)`.",
      "answer": "SQL-воронка строится по пользователям с учётом порядка событий.  \nСначала фиксируем для каждого пользователя время первого шага, затем считаем, сколько пользователей дошли до каждого следующего шага.\n\n```sql\nSELECT\n  SUM(CASE WHEN signup_time IS NOT NULL THEN 1 ELSE 0 END) AS signup,\n  SUM(CASE WHEN checkout_time IS NOT NULL THEN 1 ELSE 0 END) AS checkout,\n  SUM(CASE WHEN purchase_time IS NOT NULL THEN 1 ELSE 0 END) AS purchase\nFROM funnel;\n```",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "SQL",
        "Метрики"
      ]
    },
    {
      "id": 68,
      "question": "Что показывает отношение LTV/CAC и какие значения считаются приемлемыми? Как его рассчитать?",
      "answer": "LTV/CAC показывает, насколько прибыльно привлечение клиентов: сколько выручки клиент приносит за всё время жизни по сравнению со стоимостью его привлечения.\n\n- LTV/CAC < 1 — бизнес убыточен, привлечение не окупается\n- LTV/CAC ≈ 1–2 — на грани окупаемости\n- LTV/CAC ≥ 3 — считается здоровым значением\n\n`LTV ≈ ARPU × средняя_длительность_жизни_пользователя`     \n`CAC = маркетинговые_затраты / количество_привлечённых_клиентов`",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "Метрики"
      ]
    },
    {
      "id": 69,
      "question": "Когда стоит использовать логарифмическую шкалу?",
      "answer": "При показе данных, меняющихся на порядки, или при мультипликативном росте для лучшего сравнения относительных изменений.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Метрики"
      ]
    },
    {
      "id": 70,
      "question": "Для чего нужен COALESCE в SQL, и чем он лучше IFNULL?",
      "answer": "COALESCE возвращает первое не-NULL значение из списка аргументов, может принимать любое количество аргументов, работает одинаково в большинстве СУБД. IFNULL более простая функция, принимает ровно два аргумента, и есть не во всех СУБД (в PostgreSQL её нет)",
      "tags": [],
      "level": "lvl_1",
      "topic": [
        "SQL"
      ]
    },
    {
      "id": 71,
      "question": "Какие типы индексов бывают в SQL, и зачем аналитику о них знать?",
      "answer": "Основной тип — B-tree, он подходит для большинства запросов с `WHERE`, `JOIN` и `ORDER BY`. GIN и GiST используются для массивов, `jsonb` для диапазонов и геоданных, BRIN — для очень больших таблиц с упорядоченными данными. \n\nАналитик не выбирает и не создаёт индексы, но должен писать запросы так, чтобы база могла их использовать.",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "SQL"
      ]
    },
    {
      "id": 72,
      "question": "Что делает оператор UNION ALL в SQL?",
      "answer": "Склеивает выборки и сохраняет дубликаты. Не удаляет повторяющиеся строки, что быстрее и полезно для суммирования данных.",
      "tags": [],
      "level": "lvl_1",
      "topic": [
        "SQL"
      ]
    },
    {
      "id": 73,
      "question": "Как заменить NULL на значение по умолчанию в ANSI SQL?",
      "answer": "Использовать `COALESCE(col, 0)`  — он часть ANSI SQL, работает во всех основных СУБД, может принимать несколько аргументов, является стандартным способом замены `NULL`.",
      "tags": [],
      "level": "lvl_1",
      "topic": [
        "SQL"
      ]
    },
    {
      "id": 74,
      "question": "В чём преимущество DATE_TRUNC перед EXTRACT при построении календарей?",
      "answer": "DATE_TRUNC возвращает саму дату начала периода (неделя, месяц), которую можно использовать в GROUP BY и JOIN, а EXTRACT даёт номер периода, которые далее необходимо еще комбинировать с годом.",
      "tags": [],
      "level": "lvl_1",
      "topic": [
        "SQL"
      ]
    },
    {
      "id": 75,
      "question": "Какой оператор в SQL вернёт первые N строк без гарантии порядка?",
      "answer": "LIMIT ограничивает число строк. Порядок задаёт ORDER BY.",
      "tags": [],
      "level": "lvl_1",
      "topic": [
        "SQL"
      ]
    },
    {
      "id": 76,
      "question": "Как быстро оценить план выполнения запроса в PostgreSQL?",
      "answer": "Использовать `EXPLAIN`. Он показывает план выполнения запроса без его запуска.  \nДля анализа реального времени и затрат используют `EXPLAIN ANALYZE`, он выполняет запрос и показывает фактические метрики.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "SQL"
      ]
    },
    {
      "id": 77,
      "question": "Что такое утечка признаков в машинном обучении, и как её избежать?",
      "answer": "Утечка признаков — это ситуация, когда модель в обучении получает информацию, которая в реальном прогнозе ей недоступна (например, данные из будущего или признаки, зависящие от таргета). Это приводит к завышенному качеству на валидации и провалу в test. Избегают утечек строгим разделением данных во времени, корректным feature engineering (все агрегации — только на train), и контролем того чтобы признаки не содержали информации о целевой переменной.",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "ML"
      ]
    },
    {
      "id": 78,
      "question": "Что такое window frame и чем отличается `ROWS BETWEEN` от `RANGE BETWEEN`?",
      "answer": "Window frame задаёт набор строк, по которым считается оконная функция для текущей строки.  `ROWS BETWEEN` — считает окно по физическому количеству строк (строка выше / ниже). `RANGE BETWEEN` — считает окно по значению в `ORDER BY` (например, диапазон дат или чисел), может включать несколько строк с одинаковым значением.",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "SQL"
      ]
    },
    {
      "id": 79,
      "question": "Как оптимизировать запрос с несколькими `JOIN` и фильтрами в SQL?",
      "answer": "Сначала уменьшаем объём данных, выносим фильтры в `WHERE` и `CTE` подзапросы до `JOIN`.  Проверяем, что на ключах соединения и фильтрации есть индексы. Используем `EXPLAIN / EXPLAIN ANALYZE`, чтобы найти `Seq Scan`, тяжёлые `JOIN` и лишние шаги. Если логика сложная или используется повторно — думаем о материализации промежуточного результата.",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "SQL",
        "Алгоритмы"
      ]
    },
    {
      "id": 80,
      "question": "Что делает оператор `MERGE` в SQL, и когда его используют?",
      "answer": "`MERGE` позволяет в одном выражении выполнить `INSERT` и `UPDATE` (и при необходимости `DELETE`) в зависимости от того, найден ли ключ в целевой таблице.  \nЧасто используется при обновлении витрин и инкрементальной загрузке данных.",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "SQL"
      ]
    },
    {
      "id": 81,
      "question": "Как безопасно удалить дубликаты, оставив одну запись в SQL?",
      "answer": "Дубликаты нумеруют с помощью `ROW_NUMBER()` внутри группы одинаковых ключей, затем удаляют строки с номером больше 1. Такой подход позволяет явно контролировать, какая запись остаётся, и избежать случайного удаления.",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "SQL"
      ]
    },
    {
      "id": 82,
      "question": "Как работает оконная функция `LAG` и для чего она используется?",
      "answer": "`LAG` позволяет получить значение из предыдущей строки внутри окна без использования self join. Часто применяется для расчёта разницы между текущим и предыдущим значением, например изменения суммы покупки.",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "SQL"
      ]
    },
    {
      "id": 83,
      "question": "В чём разница между `ROWS` и `RANGE` в оконных функциях?",
      "answer": "`ROWS` задаёт окно по количеству строк, например последние 30 событий, независимо от времени между ними. `RANGE` задаёт окно по значениям в `ORDER BY` — например, все события за последние 30 минут относительно текущей строки (в него может попасть разное количество строк).\n\n```sql\navg(value) \n\tover (order by event_time \n\trows between 29 preceding and current row) \n\t-- последние 30 событий\n\navg(value) \n\tover (order by event_time \n\trange between interval '30 minutes' preceding and current row) \n\t-- события за последние 30 минут\n```",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "SQL"
      ]
    },
    {
      "id": 84,
      "question": "Как в PostgreSQL посчитать перцентиль с помощью оконной функции?",
      "answer": "В PostgreSQL для расчёта перцентилей используется агрегатная функция `percentile_cont`. В сочетании с `OVER (PARTITION BY …)` она позволяет посчитать перцентиль без `GROUP BY`, возвращая значение для каждой строки внутри группы.\n\n```sql\nSELECT\n  endpoint,\n  score,\n  percentile_cont(0.95)\n    WITHIN GROUP (ORDER BY score)\n    OVER (PARTITION BY endpoint) AS p95\nFROM events;\n```\n\n```\nendpoint | score | p95",
      "tags": [],
      "level": null,
      "topic": [
        "SQL"
      ]
    },
    {
      "id": 85,
      "question": "Что делает данный запрос? \n\n```sql\nWITH monthly AS (\n  SELECT \n\t  DATE_TRUNC('month', event_time) AS month,\n\t  COUNT(DISTINCT user_id) AS mau\n  FROM events\n  GROUP BY 1\n),\nstats AS (\n  SELECT \n\t  month,\n\t  mau,\n      AVG(mau) OVER (ORDER BY month ROWS BETWEEN 2 PRECEDING AND CURRENT ROW) AS mean_mau,\n\t  STDDEV_SAMP(mau) OVER (ORDER BY month ROWS BETWEEN 2 PRECEDING AND CURRENT ROW) AS std_mau\n  FROM monthly\n)\nSELECT \n\tmonth,\n    mau,\n    (mau - mean_mau) / NULLIF(std_mau, 0) AS z_score\nFROM stats;\n```",
      "answer": "Запрос считает MAU по месяцам, затем для каждого месяца вычисляет скользящее среднее и стандартное отклонение за последние 3 месяца. На основе этих значений рассчитывается z-score, который показывает, насколько текущий MAU отклоняется от недавнего тренда и помогает выявлять аномалии.",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "SQL"
      ]
    },
    {
      "id": 86,
      "question": "Определите плотность экспоненциального распределения и его матожидание.",
      "answer": "Экспоненциальное распределение это $X \\sim \\mathrm{Exp}(\\lambda)$, функция плотности $f(x)=λe^{-λx}$, матожидание $E[X]=\\frac{1}{\\lambda}$, дисперсия $Var[X]=\\frac{1}{\\lambda^2}$. \n\nЧаще всего экспоненциальным распределением описывают различные процессы, непрерывные во времени, но с которыми рано или поздно должно что-то случиться (время горения лампочки, время между распадом атомов, время между кликами).",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "Теория вероятности"
      ]
    },
    {
      "id": 87,
      "question": "Как получить первое и последнее значение показателя в группе в SQL?",
      "answer": "Использовать оконные функции `FIRST_VALUE` и `LAST_VALUE` с `PARTITION BY` и `ORDER BY`. Важно учитывать, что по умолчанию применяется frame `RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW`. Из-за чего `LAST_VALUE` возвращает значение текущей строки, а не последней в группе, что приводит к непредвиденному результату (строки множаться), поэтому для него обычно явно задают frame `ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING`.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "SQL"
      ]
    },
    {
      "id": 88,
      "question": "Как вычислить долю заказа пользователя от общей выручки в SQL?",
      "answer": "`amount / SUM(amount) OVER ()` — оконная сумма без `PARTITION` позволяет делить значение строки на общую выручку.",
      "tags": [],
      "level": "lvl_1",
      "topic": [
        "SQL"
      ]
    },
    {
      "id": 89,
      "question": "Как посчитать скользящую 3-дневную сумму без пропусков дат в SQL?",
      "answer": "Использовать `SUM(amount) OVER (ORDER BY date ROWS BETWEEN 2 PRECEDING AND CURRENT ROW)` - оконная функция с фреймом `ROWS BETWEEN 2 PRECEDING AND CURRENT ROW` создаёт окно из трёх дней подряд.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "SQL"
      ]
    },
    {
      "id": 90,
      "question": "Что делает данный запрос?\n\n```sql\nSELECT \n\tuser_id, \n\tCOUNT(*) FILTER (WHERE event_name = \"signup\") AS signups,\n    COUNT(*) FILTER (WHERE event_name = \"purchase\") AS purchases\nFROM events\nWHERE event_date >= current_date - INTERVAL \"30 day\"\nGROUP BY user_id;\n```",
      "answer": "Для каждого user_id покажет число регистраций и покупок за последние 30 дней, используя условные агрегаты.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "SQL"
      ]
    },
    {
      "id": 91,
      "question": "Когда стоит использовать DISTINCT, а когда GROUP BY?",
      "answer": "DISTINCT убирает дубликаты по столбцам, GROUP BY нужен, если помимо уникальных комбинаций требуется агрегировать дополнительные поля.",
      "tags": [],
      "level": "lvl_1",
      "topic": [
        "SQL"
      ]
    },
    {
      "id": 92,
      "question": "Как правильно отфильтровать агрегированные группы по условию в SQL?",
      "answer": "Применить `HAVING` после `GROUP BY`. `HAVING` применяется к уже агрегированным группам и поддерживает агрегатные функции.",
      "tags": [],
      "level": "lvl_1",
      "topic": [
        "SQL"
      ]
    },
    {
      "id": 93,
      "question": "Как получить TOP-N товаров в категории с помощью оконных функций? Есть поля `category`, `revenue`.",
      "answer": "Используем `ROW_NUMBER() OVER (PARTITION BY category ORDER BY revenue DESC)` и фильтруем по `rn <= N`",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "SQL"
      ]
    },
    {
      "id": 94,
      "question": "Что делает данный запрос?\n\n```sql\nSELECT \n\tuser_id, \n\tMIN(event_time) AS first_use\nFROM events\nWHERE event_name = ANY(ARRAY[\"export\", \"download\"])\nGROUP BY user_id \nHAVING COUNT(*) >= 2;\n```",
      "answer": "Находит пользователей, которые дважды использовали export ИЛИ download, и выводит первое срабатывание.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "SQL"
      ]
    },
    {
      "id": 95,
      "question": "Какой результат вернёт данный запрос?\n\n```sql\nWITH RECURSIVE tree AS (\n\tSELECT \n\t\tid, \n\t\tparent_id, \n\t\tname, \n\t\t1 AS depth, \n\t\tname AS path\n\tFROM categories\n\tWHERE parent_id IS NULL\n\tUNION ALL\n\tSELECT \n\t\tc.id,\n\t\tc.parent_id,\n\t\tc.name,\n\t\tt.depth + 1 AS depth,\n\t\tCONCAT(t.path, ' > ', c.name) AS path\n\tFROM categories c\n\tJOIN tree t ON c.parent_id = t.id\n\t)\nSELECT * \nFROM tree \nORDER BY path;\n```",
      "answer": "Это рекурсивный запрос (RECURSIVE CTE), вернёт полную иерархию категорий с глубиной и текстовым путём вида «Root > Subcategory».",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "SQL"
      ]
    },
    {
      "id": 96,
      "question": "Как построить product tree для категорий и посчитать суммарный вклад каждого узла в SQL?",
      "answer": "Используем `RECURSIVE CTE` для обхода дерева категорий и агрегируем продажи, суммируя дочерние узлы. Это позволит увидеть вклад каждого уровня.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "SQL"
      ]
    },
    {
      "id": 97,
      "question": "Что делает данный запрос?\n\n```sql\nSELECT \n\tDATE_TRUNC('week', created_at) AS week,\n    percentile_cont(0.9) WITHIN GROUP (ORDER BY response_minutes)\nFROM tickets\nGROUP BY 1;\n```",
      "answer": "Возвращает недельный 90-й перцентиль времени ответа.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "SQL"
      ]
    },
    {
      "id": 98,
      "question": "Чем отличаются `ROLLUP` и `CUBE` в `GROUP BY`?",
      "answer": "`ROLLUP` считает иерархические итоги — сворачивает измерения слева направо (например: регион → канал → общий итог).  `GROUP BY ROLLUP (region, channel)`\n\n`CUBE` считает все возможные комбинации измерений, включая частичные и общий итог. `CUBE` это часть ANSI SQL, но используется реже из-за взрывного роста числа строк. `GROUP BY CUBE (region, channel)`",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "SQL"
      ]
    },
    {
      "id": 99,
      "question": "Что вычисляет данный запрос? \n\n```sql\nWITH funnel AS (\n  SELECT\n\t  user_id,\n\t  MAX(event_time) FILTER (WHERE event_name = 'apply') AS applied,\n      MAX(event_time) FILTER (WHERE event_name = 'approved') AS approved,\n      MAX(event_time) FILTER (WHERE event_name = 'funded') AS funded\n  FROM credit_events\n  GROUP BY user_id\n)\nSELECT\n  COUNT(*) AS applicants,\n  COUNT(*) FILTER (WHERE approved IS NOT NULL) AS approvals,\n  COUNT(*) FILTER (WHERE funded IS NOT NULL) AS funded_cnt,\n  COUNT(*) FILTER (WHERE approved IS NOT NULL)::float / COUNT(*) AS approve_rate,\n  COUNT(*) FILTER (WHERE funded IS NOT NULL)::float / COUNT(*) AS fund_rate\nFROM funnel;\n```",
      "answer": "Данный запрос вычисляет кумулятивную конверсию кредитной воронки, строит трёхшаговую воронку и выводит абсолютные и относительные показатели для каждой ступени благодаря FILTER.",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "SQL"
      ]
    },
    {
      "id": 100,
      "question": "Когда стоит хранить агрегаты в отдельной таблице (rollups) в SQL?",
      "answer": "Rollups используют, когда метрика считается дорого и переиспользуется в нескольких отчётах. Агрегаты пересчитываются периодически и хранятся в отдельной таблице, что позволяет читать меньше данных, использовать partition pruning и снижать нагрузку на сырые события.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "SQL"
      ]
    },
    {
      "id": 101,
      "question": "Почему оконные функции удобнее self join для поиска предыдущего события?",
      "answer": "`LAG`/`LEAD` обращаются к соседним строкам без дополнительных связок и читаются проще. Self join требует агрегатов и условий, что усложняет производительность.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "SQL"
      ]
    },
    {
      "id": 102,
      "question": "Когда полезно использовать `USING` вместо `ON` в `JOIN` в SQL?",
      "answer": "`USING` применяют, когда таблицы соединяются по колонке с одинаковым именем.  \nОн упрощает синтаксис `JOIN`, автоматически убирает дублирующийся столбец из результата и снижает риск опечаток по сравнению с `ON`.\n\n```sql\nSELECT *\nFROM users\nJOIN orders USING (user_id);\n```",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "SQL"
      ]
    },
    {
      "id": 103,
      "question": "Как объяснить работу `GROUPING SETS` в SQL простыми словами?",
      "answer": "`GROUPING SETS` позволяют **посчитать несколько разных `GROUP BY` за один запрос**, вместо того чтобы писать несколько запросов и склеивать их через `UNION ALL`.\n\n```sql\nSELECT country, device, SUM(revenue) AS total_revenue\nFROM sales\nGROUP BY GROUPING SETS ((country), (device), (country, device));\n```",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "SQL"
      ]
    },
    {
      "id": 104,
      "question": "Что делает `DISTINCT ON` в PostgreSQL?",
      "answer": "`DISTINCT ON` в PostgreSQL позволяет выбрать по одной строке на ключ, управляя выбором через `ORDER BY`. Поля в `DISTINCT ON (...)` должны идти первыми в `ORDER BY`.   \n\nПример, для каждого пользователя выбрать последний заказ:\n\n```sql\nSELECT DISTINCT ON (user_id)\n       user_id,\n       order_id,\n       amount,\n       created_at\nFROM orders\nORDER BY user_id, created_at DESC;\n```",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "SQL"
      ]
    },
    {
      "id": 105,
      "question": "Что делает оператор `LATERAL` в SQL?",
      "answer": "`LATERAL` обеспечивает доступ к колонкам внешнего запроса внутри подзапроса.\n\n```sql\nSELECT\n  u.name,\n  o.amount\nFROM users u\nLEFT JOIN LATERAL (\n  SELECT amount\n  FROM orders\n  WHERE orders.user_id = u.id\n  ORDER BY amount DESC\n  LIMIT 1\n) o ON true;\n```",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "SQL"
      ]
    },
    {
      "id": 106,
      "question": "Как в SQL посчитать D1 retention с разрезом по типу устройства?",
      "answer": "Для расчёта D1 retention сначала формируют когорту пользователей по дате первого события (например, `MIN(event_date)` по `user_id`). Затем эти пользователи джойнятся с событиями на следующий день и считается доля пользователей, у которых была активность. Группировка по `device_type` позволяет сравнить retention между платформами.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "SQL"
      ]
    },
    {
      "id": 107,
      "question": "Как построить витрину product-health c KPI по странам в SQL?",
      "answer": "Сначала считают ключевые метрики по странам (`activation`, `retention`, `revenue`). Затем метрики нормируют (например, через z-score), чтобы привести их к одному масштабу. Итоговый `health_score` рассчитывается как взвешенная сумма нормированных метрик.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "SQL"
      ]
    },
    {
      "id": 108,
      "question": "Как найти пользователей, у которых снизилась глубина сессии events_per_session на 30% в SQL?",
      "answer": "Посчитать средний events_per_session за последние 7 и предыдущие 7 дней, добавить `CASE WHEN current < previous * 0.7 THEN 1 END`, и агрегигация.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "SQL"
      ]
    },
    {
      "id": 109,
      "question": "Как построить витрину для расчёта MDE по историческим данным?",
      "answer": "Для расчёта MDE используют витрину с историческими статистиками целевой метрики: средним значением и стандартным отклонением, обычно агрегированными по неделям. Эти значения хранятся в отдельной таблице и используются в калькуляторе MDE для оценки минимального обнаружимого эффекта при заданном размере выборки и уровне значимости.",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "SQL"
      ]
    },
    {
      "id": 110,
      "question": "Что такое MDE",
      "answer": "MDE, (Minimum Detectable Effect)  — это минимальный эффект, который эксперимент способен надёжно обнаружить. \n\nНапример: «рост конверсии минимум на +0.5 п.п.». Он зависит от дисперсии метрики $σ$, размера выборки $n$, уровня значимости $α$, мощности теста $1 − β$.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "A/B тесты"
      ]
    },
    {
      "id": 111,
      "question": "Зачем использовать `LISTAGG` / `STRING_AGG` в аналитических задачах в SQL?",
      "answer": "`LISTAGG` и `STRING_AGG` агрегируют значения нескольких строк в одну строку внутри группы. Они полезны, когда нужно компактно показать список элементов, например действия пользователя, состав заказа или набор категорий в одной строке.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "SQL"
      ]
    },
    {
      "id": 112,
      "question": "В чём разница между `UNION` и `UNION ALL` в SQL, и почему в витринах данных чаще используют `UNION ALL`?",
      "answer": "`UNION` удаляет дубликаты, из-за чего требует дополнительной сортировки или агрегации и работает медленнее. `UNION ALL` просто объединяет наборы строк без удаления дублей и поэтому быстрее. В ETL и витринах обычно используют `UNION ALL`, а дедупликацию выполняют отдельным, контролируемым шагом.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "SQL"
      ]
    },
    {
      "id": 113,
      "question": "Что такое ETL",
      "answer": "ETL (Extract → Transform → Load) — это процесс подготовки данных. Extract — забрали данные из источников (логи, БД, API). Transform — почистили, привели к нужному виду, агрегировали. Load — загрузили в витрину.  \n    \nВ аналитике ETL — это обновление витрин, пересчёт метрик, регулярные SQL-скрипты.",
      "tags": [],
      "level": "lvl_1",
      "topic": [
        "SQL"
      ]
    },
    {
      "id": 114,
      "question": "Почему стоит явно задавать `COLLATE` при сравнении строк в SQL?",
      "answer": "`COLLATE` определяет правила сравнения и сортировки строк (регистр, порядок символов). Так как дефолтная коллация может отличаться между БД и колонками, явное указание `COLLATE` делает поведение запросов предсказуемым и помогает корректно использовать строковые индексы.\n\n`=` и `LIKE / ILIKE` — это операторы сравнения; `COLLATE` — это настройка того, как именно строки считаются равными и сортируются.",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "SQL"
      ]
    },
    {
      "id": 115,
      "question": "Почему важно нормализовать timezone при анализе событий?",
      "answer": "События могут приходить в локальном времени. Приведение к UTC и наличие отдельного столбца user_timezone предотвращают сдвиги метрик и дублирование суток.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "SQL"
      ]
    },
    {
      "id": 116,
      "question": "Как найти пользователей, которые активировали фичу в течение 3 дней после регистрации в SQL?",
      "answer": "Сравниваем `feature_date`  с `signup_date` через `CASE WHEN feature_date BETWEEN signup_date AND signup_date + 3 THEN 1 END`, и агрегируем на уровне пользователя.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "SQL"
      ]
    },
    {
      "id": 117,
      "question": "Как симулировать распределение статистики и оценить вероятность ошибки II рода?",
      "answer": "Ошибка II рода (β) — это вероятность не обнаружить эффект, когда он на самом деле есть.\n\nЧтобы её оценить, нужно:\n1. Задать альтернативную гипотезу (реальный эффект).\n2. Сгенерировать много выборок при наличии эффекта.\n3. Для каждой выборки посчитать статистику теста.\n4. Посчитать долю случаев, когда статистика не попала в критическую область.\n    \nЭта доля и есть вероятность ошибки II рода (β).  \nМощность теста в таком случае равна (1 − β).",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "Статистика"
      ]
    },
    {
      "id": 118,
      "question": "Что такое переобучение и как его обнаружить?",
      "answer": "Переобучение (overfitting) — это ситуация, когда модель слишком хорошо подстраивается под обучающую выборку, но плохо обобщает знания на новые данные. Если модель «знает ответы наизусть», но ошибается на новых данных — это переобучение. Чтобы обнаружить переобучение сравнивают train и val, анализируют learning curves.",
      "tags": [],
      "level": "lvl_1",
      "topic": [
        "ML"
      ]
    },
    {
      "id": 119,
      "question": "Что такое регуляризация и какие её виды существуют?",
      "answer": "Регуляризация — это набор методов, которые ограничивают сложность модели, чтобы снизить переобучение и улучшить обобщающую способность. Основные способы регуляризации:\n- L1 (Lasso) — штрафует сумму модулей весов, может занулять параметры\n- L2 (Ridge) — штрафует сумму квадратов весов, сглаживает модель\n- Elastic Net — комбинация L1 и L2\n- Dropout — случайно отключает нейроны при обучении\n- Early Stopping — останавливает обучение до переобучения\n- Data Augmentation — увеличивает разнообразие данных",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "ML"
      ]
    },
    {
      "id": 120,
      "question": "Объясните разницу между Precision и Recall.",
      "answer": "Precision и Recall это метрики качества в бинарной классификации, которые оцениваются относительно положительного класса (`TP`). \n\nPrecision (точность) - показывает количество правильно предсказанных единичек (`TP`) среди всех ПРЕДСКАЗАННЫХ единичек. `Precision = TP / (TP + FP)`. \n\nRecall (полнота) - показывает количество правильно предсказанных единичек (`TP`) среди всех ИСТИННЫХ единичек.  `Recall = TP / (TP + FN)`.\n\n|            | true 1   | true 0 |             |\n|",
      "tags": [],
      "level": null,
      "topic": [
        "ML"
      ]
    },
    {
      "id": 121,
      "question": "Когда ROC-кривая лучше PR-кривой и наоборот?",
      "answer": "ROC (Receiver Operating Characteristic) - это график, который показывает качество бинарного классификатора при разных порогах. Лучше подходит, когда: классы примерно сбалансированы, важна общая способность модели различать классы, ложноположительные и ложноотрицательные ошибки имеют схожую цену\n\nPR-кривая предпочтительнее, когда: данные сильно не сбалансированы, положительный класс редкий и важный, критично качество предсказаний положительного класса (precision, recall)\n\nROC — про разделимость классов в целом.   \nPR — про качество работы с редким положительным классом.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "ML"
      ]
    },
    {
      "id": 122,
      "question": "Что такое градиентный бустинг?",
      "answer": "Градиентный бустинг — это ансамблевый метод, который строит модель поэтапно, добавляя слабые модели так, чтобы каждая новая модель исправляла ошибки предыдущих, двигаясь в сторону минимума функции потерь.  \n\nГрадиентный спуск — это способ минимизировать функцию потерь.    \nГрадиентный бустинг — это способ построить модель, добавляя слабые модели по градиенту ошибки.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "ML"
      ]
    },
    {
      "id": 123,
      "question": "Как оценивают важность признаков в деревьях решений?",
      "answer": "Важность признаков в деревьях решений оценивают несколькими способами: \n- Impurity-based importance — измеряет, насколько признак уменьшает неопределённость при разбиениях; \n- Permutation importance — оценивает падение качества модели при случайной перестановке значений признака; \n- SHAP values — показывает вклад признака в предсказание для отдельных объектов и в среднем по выборке. \n\nВажный признак — тот, без которого модель начинает хуже предсказывать.",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "ML"
      ]
    },
    {
      "id": 124,
      "question": "Что такое baseline модель?",
      "answer": "Это самая простая модель (например, среднее или медиана), служащая отправной точкой для оценки дальнейших улучшений.",
      "tags": [],
      "level": "lvl_1",
      "topic": [
        "ML"
      ]
    },
    {
      "id": 125,
      "question": "Как работает кросс-валидация и зачем она нужна?",
      "answer": "Кросс-валидация — это способ оценки качества модели, при котором данные делят на `k` частей (фолдов). Модель обучают `k` раз: каждый раз один фолд используется для валидации, а остальные — для обучения. Кросс-валидация нужна, чтобы: получить более стабильную оценку качества, уменьшить зависимость результата от случайного разбиения данных, лучше оценить обобщающую способность модели.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "ML"
      ]
    },
    {
      "id": 126,
      "question": "Когда линейная регрессия неприменима?",
      "answer": "Линейная регрессия становится неприемлемой, когда её базовые предположения нарушены и это нельзя исправить простыми приёмами. Основные случаи:\n\n- Нелинейная связь между признаками и целевой переменной\n- Сильная мультиколлинеарность, делающая коэффициенты нестабильными\n- Гетероскедастичность, при которой дисперсия ошибок зависит от признаков\n- Ненормальность остатков, если она критична для выводов и доверительных интервалов\n\nЕсли данные ведут себя не линейно — линейная регрессия начинает врать.  \nВ таких ситуациях применяют другие модели.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "ML"
      ]
    },
    {
      "id": 127,
      "question": "Что такое Гетероскедастичность?",
      "answer": "Гетероскедастичность — это когда разброс ошибок НЕ одинаков. Например предсказываем доход по стажу: стаж 1-2 года → доходы ±5k, стаж 20 лет → доходы ±100k. Разброс и ошибки растут вместе с признаком, в нашем примере со стажем. \n\nЛинейная регрессия предполагает что ошибки имеют постоянную дисперсию, или отсутствие Гетероскедастичности.",
      "tags": [],
      "level": "lvl_1",
      "topic": [
        "Статистика"
      ]
    },
    {
      "id": 128,
      "question": "Что такое Логистическая регрессия?",
      "answer": "Логистическая регрессия — это линейная модель классификации, которая оценивает вероятность принадлежности объекта к классу с помощью взвешенной суммы признаков и сигмоидной функции. Проводит «линию» (или гиперплоскость) между классами и говорит, по какую сторону объект находится.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "ML"
      ]
    },
    {
      "id": 129,
      "question": "Что такое Дерево решений?",
      "answer": "Дерево решений — это нелинейная модель, которая последовательно делит данные на группы с помощью правил вида «если признак ≤ значение». Задаёт цепочку вопросов и по ним принимает решение. Концептуально эквивалентно тернарным операторам (if, elif, else) в коде, но правила и пороги выбираются алгоритмом, а не в ручную.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "ML"
      ]
    },
    {
      "id": 130,
      "question": "Когда логистическая регрессия предпочтительнее дерева решений?",
      "answer": "Логистическая регрессия предпочтительнее, когда: классы примерно линейно разделимы, важна интерпретируемость и влияние каждого признака, данные зашумлены и нужно устойчивое решение, требуется стабильная воспроизводимая модель. Деревья решений лучше подходят для сложных нелинейных зависимостей, но они чаще переобучаются.\n\nЛогистическая регрессия — простая и стабильная.    \nДерево — гибкое, но легко переобучается.",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "ML"
      ]
    },
    {
      "id": 131,
      "question": "Что такое логистическая функция?",
      "answer": "Логистическая функция (sigmoid) — это функция, которая преобразует любое вещественное число в значение от 0 до 1. Обычно используется для интерпретации результата модели как вероятности. Формула: $σ(x) = \\frac{1}{1 \\ + \\ e^{−x}}$. Применяется в Логистической регрессии. \n\nЛогистическая функция «сжимает» любое число в диапазон вероятностей от 0 до 1.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "ML"
      ]
    },
    {
      "id": 132,
      "question": "Как бороться с дисбалансом классов?",
      "answer": "С дисбалансом классов борются, чтобы модель не игнорировала редкий, но важный класс. Основные подходы:\n- Взвешивание классов — увеличивает штраф за ошибки на редком классе\n- Oversampling — увеличивает число объектов редкого класса\n- Undersampling — уменьшает число объектов частого класса\n- Корректные метрики — используют precision, recall, F1, PR-AUC вместо accuracy",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "ML"
      ]
    },
    {
      "id": 133,
      "question": "Для чего нужен валидационный набор данных?",
      "answer": "Валидационный набор используется для оценки модели в процессе обучения и подбора гиперпараметров. Нужен, чтобы настраивать параметры модели, контролировать переобучение, выбирать момент остановки обучения (early stopping), сравнивать разные модели и конфигурации.",
      "tags": [],
      "level": "lvl_1",
      "topic": [
        "ML"
      ]
    },
    {
      "id": 134,
      "question": "Что показывает Learning curve?",
      "answer": "Learning curve — это график зависимости качества модели от размера выборки. По learning curve можно понять: хватает ли данных для обучения, есть ли переобучение или недообучение, поможет ли добавление новых данных, ограничено ли качество моделью или данными.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "ML"
      ]
    },
    {
      "id": 135,
      "question": "Что делает k-fold кросс-валидация?",
      "answer": "k-fold кросс-валидация делит данные на k частей (фолдов) и обучает модель k раз, каждый раз используя один фолд для валидации, а остальные — для обучения. Метод позволяет: усреднить качество по разным разбиениям, снизить дисперсию оценки, получить более надёжную оценку модели.\n\nКросс-валидация — общее понятие (делить данные, обучать модель, усреднять качество).    \nk-fold — частный случай кросс-валидации",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "ML"
      ]
    },
    {
      "id": 136,
      "question": "Чем отличается дерево решений от случайного леса?",
      "answer": "Дерево решений — это одна модель, которая последовательно делит данные по правилам. Она проста и интерпретируема, но легко переобучается. \n\nСлучайный лес — это ансамбль деревьев решений, обученных на разных подвыборках данных и случайных подмножествах признаков. Такой подход снижает переобучение и делает модель более устойчивой, но также делает ее менее интерпретируемой.\n\nОдно дерево — быстро и понятно, но нестабильно.  \nЛес — надёжнее за счёт усреднения многих деревьев.",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "ML"
      ]
    },
    {
      "id": 137,
      "question": "Как устроен градиентный бустинг?",
      "answer": "Градиентный бустинг — это ансамблевый метод, в котором модели обучаются последовательно. Каждая новая модель обучается на ошибках предыдущих, аппроксимируя отрицательный градиент функции потерь и тем самым уменьшая общий лосс. \n\nГрадиентный бустинг — это последовательное исправление ошибок модели шаг за шагом в сторону минимума функции потерь.",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "ML"
      ]
    },
    {
      "id": 138,
      "question": "Что такое регуляризация в линейной модели?",
      "answer": "Регуляризация — это добавление штрафа к функции потерь, которое ограничивает величину коэффициентов модели и снижает переобучение. Основные виды:\n- L1 (Lasso) — штрафует сумму модулей весов, может занулять коэффициенты\n- L2 (Ridge) — штрафует сумму квадратов весов, делает веса меньше и стабильнее\n    \nРегуляризация не даёт коэффициентам “раздуваться” и подгоняться под шум, переобучаться.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "ML"
      ]
    },
    {
      "id": 139,
      "question": "Как интерпретировать коэффициенты логистической регрессии?",
      "answer": "Коэффициенты логистической регрессии показывают, какие признаки и в какую сторону влияют на вероятность положительного класса, и насколько сильно.\n- положительный коэффициент - вероятность класса 1 растёт\n- отрицательный коэффициент - вероятность класса 1 падает\n- модуль коэффициента - сила влияния признака",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "ML"
      ]
    },
    {
      "id": 140,
      "question": "Как интерпретировать Shapley (SHAP) values?",
      "answer": "SHAP values показывают, какой вклад каждый признак внёс в предсказание модели для конкретного объекта по сравнению с базовым значением. Интерпретация:\n- положительное SHAP-values — признак увеличил предсказание\n- отрицательное SHAP-values — признак уменьшил предсказание\n- сумма SHAP-values всех признаков равна разнице между предсказанием и базовым уровнем\n    \nSHAP основан на идее справедливого распределения вклада, учитывает все возможные комбинации признаков, поэтому на практике используется в виде приближений. SHAP отвечает на вопрос: “кто и на сколько повлиял на это предсказание?”",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "ML"
      ]
    },
    {
      "id": 141,
      "question": "Что такое softmax в машинном обучении?",
      "answer": "Softmax — это функция, которая превращает набор чисел в вероятностное распределение по классам. Используется, когда: классов больше двух; нужно выбрать один из нескольких вариантов.\n    \nSoftmax делает:\n- все значения становятся положительными\n- их сумма равна 1\n- результат можно интерпретировать как вероятность каждого класса\n    \nОтвечает на вопрос:  «к какому из нескольких классов объект наиболее вероятно относится?». В GPT softmax используется для получения вероятностей следующего токена.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "ML"
      ]
    },
    {
      "id": 142,
      "question": "В чём разница между bagging и boosting?",
      "answer": "Bagging (Bootstrap Aggregating) обучает несколько моделей параллельно на разных выборках и усредняет их предсказания. Основная цель — снизить дисперсию и переобучение.\n\nBoosting обучает модели последовательно: каждая новая модель фокусируется на ошибках предыдущих. Основная цель — уменьшить смещение (bias) и повысить качество.\n\nBagging — много независимых мнений и усреднение.  \nBoosting — поэтапное исправление ошибок.",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "ML"
      ]
    },
    {
      "id": 143,
      "question": "Зачем стандартизировать признаки перед PCA?",
      "answer": "PCA чувствителен к масштабу признаков, так как ищет направления с максимальной дисперсией. Если признаки измеряются в разных единицах, признаки с большой дисперсией будут доминировать и искажать главные компоненты. Стандартизация (приведение к нулевому среднему и единичной дисперсии) делает вклад признаков сопоставимым и позволяет PCA отражать реальную структуру данных, а не различия в масштабе.",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "ML"
      ]
    },
    {
      "id": 144,
      "question": "Как работает list comprehension в Python, и чем он лучше цикла?",
      "answer": "List comprehension — это краткий синтаксис создания списка на основе другого итерируемого объекта с возможной фильтрацией и преобразованием элементов.\n\n- код становится короче и читабельнее\n- операции выполняются быстрее за счёт оптимизаций на уровне интерпретатора\n- удобно выражать простые преобразования коллекций\n    \nList comprehension лучше использовать для простых выражений, сложную логику — оставлять в циклах.",
      "tags": [],
      "level": "lvl_1",
      "topic": [
        "Python"
      ]
    },
    {
      "id": 145,
      "question": "Чем полезен `enumerate` при обходе списка?",
      "answer": "`enumerate` позволяет итерироваться по коллекции, одновременно получая индекс и значение элемента.",
      "tags": [],
      "level": "lvl_1",
      "topic": [
        "Python"
      ]
    },
    {
      "id": 146,
      "question": "Какой тип коллекции в Python является неизменяемым?",
      "answer": "`tuple` — кортеж, неизменяемая коллекция в Python, после создания его элементы нельзя изменить, добавить или удалить.\n\n`frozenset` — неизменяемая версия множества\n    \n`str` — строка (последовательность символов)",
      "tags": [],
      "level": "lvl_1",
      "topic": [
        "Python"
      ]
    },
    {
      "id": 147,
      "question": "Как проверить, содержится ли ключ в словаре без выброса исключения в Pythone?",
      "answer": "Оператор `in` в конструкции `key in my_dict` проверяет наличие ключа по хеш-таблице.",
      "tags": [],
      "level": "lvl_1",
      "topic": [
        "Python"
      ]
    },
    {
      "id": 148,
      "question": "Как удалить дубликаты в Pandas?",
      "answer": "Для удаления дубликатов в Pandas используют метод `drop_duplicates()`. Удаляет дубликаты по всем столбцам, дубликаты по выбранным столбцам (`subset`), оставляет первое или последнее вхождение.",
      "tags": [],
      "level": "lvl_1",
      "topic": [
        "Python"
      ]
    },
    {
      "id": 149,
      "question": "Как устроен GIL и почему он не даёт линейного ускорения в потоках?",
      "answer": "GIL (Global Interpreter Lock) — это механизм в CPython, который позволяет исполняться только одному байткоду Python одновременно.\n\nИз-за GIL — CPU-bound задачи в потоках не ускоряются, потоки по очереди получают доступ к интерпретатору.\n\nGIL мешает параллельным вычислениям CPU, но не мешает ожиданию I/O. Для реального параллелизма CPU-bound задач используют multiprocessing.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Python"
      ]
    },
    {
      "id": 150,
      "question": "Зачем использовать `typing` в Python, и чем он полезен аналитикам?",
      "answer": "`typing` позволяет явно указывать типы переменных, аргументов функций и возвращаемых значений.\n\nОн помогает аналитикам:\n- улучшает читаемость и самодокументируемость кода\n- дает более точное автодополнение в IDE\n- находит ошибки типов до выполнения кода\n- упрощает code review и поддержку аналитических скриптов",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Python"
      ]
    },
    {
      "id": 151,
      "question": "Чем отличается `asyncio` от `multiprocessing` в ETL-конвейере?",
      "answer": "`asyncio` используется для параллельной обработки I/O-bound шагов (запросы к API, БД, файлам) в одном процессе. Оно эффективно скрывает задержки ввода/вывода, но не ускоряет вычисления.\n  \n`multiprocessing` запускает несколько процессов и подходит для CPU-bound шагов, так как не ограничен GIL и позволяет задействовать несколько ядер.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Python"
      ]
    },
    {
      "id": 152,
      "question": "Как уменьшить использование памяти при обработке CSV в Pandas?",
      "answer": "Чтобы снизить потребление памяти при чтении CSV в Pandas:\n\n- Явно указать `dtypes` — предотвращает автоматическое приведение к более тяжёлым типам\n- Использовать `chunksize` — читать файл частями и обрабатывать потоково\n- Использовать `usecols` — загружать только нужные столбцы\n- По возможности приводить строки к `category`",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Python"
      ]
    },
    {
      "id": 153,
      "question": "Как работает `functools.lru_cache` в Python?",
      "answer": "`lru_cache` — это декоратор, который кэширует результаты вызовов функции по её аргументам и возвращает сохранённый результат при повторном вызове.\n\n- использует стратегию LRU (Least Recently Used) — вытесняет давно неиспользуемые значения\n- сильно ускоряет повторяющиеся вычисления, особенно полезен в рекурсии\n- размер кэша можно ограничить параметром `maxsize`\n    \nПрименяется, когда функция детерминирована и часто вызывается с одинаковыми аргументами.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Python"
      ]
    },
    {
      "id": 154,
      "question": "Чем отличаются списки и кортежи в Python?",
      "answer": "Списки (`list`) — изменяемые последовательности. Элементы можно добавлять, удалять и менять. Они имеют больше методов и подходят для работы с данными которые меняются.\n\nКортежи (`tuple`) — неизменяемые последовательности. После создания их нельзя изменить. Они занимают меньше памяти, быстрее обрабатываются и могут использоваться как ключи словаря (если все элементы этого кортежа хэшируемые).",
      "tags": [],
      "level": "lvl_1",
      "topic": [
        "Python"
      ]
    },
    {
      "id": 155,
      "question": "Что такое генератор и чем он отличается от списка в Python?",
      "answer": "Генератор — это итерируемый объект, который вычисляет элементы по одному при обходе, не сохраняя их все в памяти.\n\nОтличия от списка:\n\n- генератор ленивый и экономит память\n- список вычисляет и хранит все элементы сразу\n- генератор можно пройти только один раз\n    \nГенераторы удобны для обработки больших или бесконечных последовательностей.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Python"
      ]
    },
    {
      "id": 156,
      "question": "В чём разница между `copy` и `deepcopy` в Python?",
      "answer": "`copy` создаёт поверхностную копию объекта: копируется только сам контейнер, а вложенные объекты остаются общими по ссылке.\n\n`deepcopy` создаёт глубокую копию: рекурсивно копирует все вложенные объекты, формируя полностью независимую структуру данных.\n\n`deepcopy` используют, когда нужно изменить копию, не затрагивая исходный объект.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Python"
      ]
    },
    {
      "id": 157,
      "question": "Когда использовать `pandas.merge`, а когда `pandas.join` в Python?",
      "answer": "`merge` — универсальный способ объединения DataFrame:\n- объединяет по столбцам, даже если их имена различаются\n- подходит для большинства SQL-подобных join’ов (`inner`, `left`, `right`, `outer`)\n- явно указывает ключи (`left_on`, `right_on`)\n    \n`join` — упрощённый синтаксис:\n- по умолчанию объединяет по индексам\n- удобен, когда ключ уже является индексом\n- короче и читабельнее в простых случаях",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Python"
      ]
    },
    {
      "id": 158,
      "question": "Как работает `itertools.groupby` в Python, и в чём его ограничение?",
      "answer": "`itertools.groupby` группирует последовательно идущие элементы по значению ключа и возвращает пары (ключ, итератор по группе).\n\nОграничения:\n- группирует только соседние элементы, а не все одинаковые значения\n- для корректной глобальной группировки данные нужно предварительно отсортировать\n- возвращаемые группы — итераторы, которые нужно потреблять сразу\n    \n`groupby` работает как «сжатие подряд идущих значений», а не как SQL GROUP BY.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Python"
      ]
    },
    {
      "id": 159,
      "question": "В чём разница между итератором и генератором в Python?",
      "answer": "Итератор — это объект, который реализует протокол итерации (`__iter__()` и `__next__()`) и возвращает элементы по одному при обходе.\n\nГенератор — это удобный способ создания итератора, который определяется функцией с `yield`, автоматически хранит состояние между вызовами, не требует явной реализации `__next__()`.\n    \nИтератор — это интерфейс.   \nГенератор — это частный случай итератора, реализованный проще и компактнее.",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "Python"
      ]
    },
    {
      "id": 160,
      "question": "Как быстро собрать интерактивный прототип дашборда без BI-систем в Python?",
      "answer": "Для быстрого прототипирования интерактивных дашбордов используют Python-фреймворки Dash, Streamlit или Panel.\n\nUI собираем во фреймворке, логику расчёта метрик держим отдельно, кешируем тяжёлые вычисления, фиксируем зависимости в `requirements.txt`.\n    \nТакой подход позволяет быстро показать результат и безболезненно перенести прототип в прод.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Python"
      ]
    },
    {
      "id": 161,
      "question": "Как безопасно работать с конфиденциальными переменными окружения?",
      "answer": "Конфиденциальные данные (API-ключи, пароли, токены) хранят в переменных окружения, а не в коде. Рекумендуется:\n\n- использовать `.env` и загрузку через `dotenv` или системные секреты\n- не коммитить `.env` в репозиторий\n- получать значения через `os.environ`\n- добавлять `.env.example` с перечнем переменных без значений",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Python"
      ]
    },
    {
      "id": 162,
      "question": "Какой модуль используют для сериализации объектов в JSON в Python?",
      "answer": "Для сериализации данных в JSON используют стандартный модуль `json`, это стандартный способ превратить Python-данные в формат для обмена и хранения.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Python"
      ]
    },
    {
      "id": 163,
      "question": "Как посчитать среднее значение столбца в Pandas без учёта NaN?",
      "answer": "В pandas метод `mean()` по умолчанию игнорирует NaN. При необходимости поведение можно зафиксировать явно: `df['col'].mean(skipna=True)`. Это гарантирует, что пропущенные значения не участвуют в расчёте среднего.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Python"
      ]
    },
    {
      "id": 164,
      "question": "Как зафиксировать версию зависимостей для проекта?",
      "answer": "`pip freeze > requirements.txt` — выписывает фактические версии установленных пакетов в файл requirements.txt.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Python"
      ]
    },
    {
      "id": 165,
      "question": "Чем отличается `loc` от `iloc` в Pandas?",
      "answer": "`loc` использует метки строк и столбцов: `df.loc['A', 'value']`. `iloc` использует целочисленные позиции: `df.iloc[0, 1]`",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Python"
      ]
    },
    {
      "id": 166,
      "question": "Как безопасно открывать и закрывать файлы в Python?",
      "answer": "Для безопасной работы с файлами используют контекстный менеджер `with`. Он гарантирует, что файл будет автоматически закрыт, даже если во время работы возникнет ошибка.\n\n```python\nwith open('data.txt', 'r') as f:\n    data = f.read()\n```",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Python"
      ]
    },
    {
      "id": 167,
      "question": "Что происходит при использовании mutable default аргумента в Python?",
      "answer": "Если в качестве значения по умолчанию используется изменяемый объект, он создаётся один раз при объявлении функции и переиспользуется во всех последующих вызовах. Поэтому изменения (например, `append`) накапливаются между вызовами функции, что часто приводит к неожиданному поведению. Правильный подход — использовать `None` и создавать объект внутри функции.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Python"
      ]
    },
    {
      "id": 168,
      "question": "Как работает `zip_longest` из `itertools`?",
      "answer": "`zip_longest` объединяет элементы нескольких итерируемых объектов по индексам и продолжается до самого длинного из них. Если элементы в одном из итерируемых закончились, вместо них подставляется значение `fillvalue`.\n\n```python\nzip_longest([1, 2], ['a'], fillvalue='-')  # (1, 'a'), (2, '-')\n```",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Python"
      ]
    },
    {
      "id": 169,
      "question": "Как работает `Counter.most_common`?",
      "answer": "Метод `most_common(n)` возвращает `n` элементов с наибольшей частотой из объекта `Counter` в виде пар `(элемент, количество)`. Элементы отсортированы по убыванию частоты.\n\n```python\nCounter('abbccc').most_common(2) # [('c', 3), ('b', 2)]\n```",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Python"
      ]
    },
    {
      "id": 170,
      "question": "Как работает `pathlib` при построении путей в Python?",
      "answer": "Модуль `pathlib` позволяет строить файловые пути с помощью объектов `Path`. Оператор `/` соединяет части пути, автоматически подставляя корректный разделитель для текущей ОС. Избавляет от ручной сборки путей и ошибок с разделителями.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Python"
      ]
    },
    {
      "id": 171,
      "question": "Что происходит при использовании `Decimal` для денежных расчётов в Python?",
      "answer": "Тип `Decimal` хранит числа в точном десятичном виде, поэтому операции с денежными суммами выполняются без ошибок округления, характерных для `float`.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Python"
      ]
    },
    {
      "id": 172,
      "question": "Что происходит при использовании `pandas.assign`?",
      "answer": "Метод `assign` добавляет новый столбец в DataFrame и возвращает копию, не изменяя исходный объект без явного присваивания. Удобно использовать в цепочках преобразований данных.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Python"
      ]
    },
    {
      "id": 173,
      "question": "Что делает метод `DataFrame.pipe`?",
      "answer": "Метод `pipe` позволяет применять функции к DataFrame внутри цепочки вызовов, передавая DataFrame в функцию как аргумент.\n\nЭто упрощает построение читаемых конвейеров обработки данных и разделение логики на шаги.\n\n`pipe` — это способ писать pandas-код как последовательный конвейер шагов.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Python"
      ]
    },
    {
      "id": 174,
      "question": "Что делает `asyncio.gather` в Python?",
      "answer": "`asyncio.gather()` запускает несколько корутин (управляемая функция, которая умеет ждать) конкурентно и ожидает их завершения, возвращая список результатов в порядке передачи.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Python"
      ]
    },
    {
      "id": 175,
      "question": "Как работает `csv.DictWriter` при записи отчётов в Python?",
      "answer": "`csv.DictWriter` записывает CSV-файл из списка словарей, где ключи соответствуют именам столбцов. Полезен когда данные уже в виде словарей и важны имена колонок.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Python"
      ]
    },
    {
      "id": 176,
      "question": "В чём разница между `multiprocessing` и `threading` для аналитических задач?",
      "answer": "`threading` запускает несколько потоков в одном процессе и подходит для I/O-bound задач, так как в CPython потоки ограничены GIL и не ускоряют вычисления.\n\n`multiprocessing` создаёт отдельные процессы, каждый со своим интерпретатором, что позволяет параллелить CPU-bound расчёты, но требует больше памяти и накладных расходов.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Python"
      ]
    },
    {
      "id": 177,
      "question": "Зачем использовать `pyarrow` / `parquet` при выгрузке данных в Python?",
      "answer": "Parquet — колоночный формат хранения данных со сжатием и явной схемой, оптимизированный для аналитики. PyArrow — это Python-библиотека для работы с форматом Parquet. \n\nИспользование `pyarrow` и Parquet позволяет: значительно уменьшить размер данных; ускорить чтение и запись больших таблиц; читать только нужные столбцы; легко интегрироваться с Pandas и аналитическими движками.\n    \nПо сравнению с CSV, Parquet быстрее, компактнее и лучше масштабируется.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Python"
      ]
    },
    {
      "id": 178,
      "question": "Когда стоит предпочесть векторизованный Pandas API вместо `apply`?",
      "answer": "Векторизованные операции в Pandas выполняются на уровне C/NumPy и обрабатывают массивы целиком, поэтому они значительно быстрее, чем `apply` с Python-функцией по строкам. \n\n`apply` стоит использовать только тогда, когда логику невозможно выразить векторизованно, требуется сложная Python-логика для каждой строки\n    \nВо всех остальных случаях предпочтительны векторизованные операции.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Python"
      ]
    },
    {
      "id": 179,
      "question": "Что даёт `typing.Literal` в Python, и когда его применять?",
      "answer": "`typing.Literal` позволяет ограничить значение аргумента конкретным набором допустимых значений (строк, чисел, булевых).\n\nИспользуется, когда параметр может принимать строго фиксированные варианты (режимы, флаги), важно избежать опечаток и невалидных значений.\n    \n `Literal` — это самозапрет типа «разрешены только эти значения».",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Python"
      ]
    },
    {
      "id": 180,
      "question": "Когда следует использовать `pathlib` вместо `os.path`?",
      "answer": "`pathlib` стоит использовать в большинстве новых проектов, когда нужна удобная и безопасная работа с путями.\n\n`os.path` чаще встречается в легаси-коде или низкоуровневых скриптах.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Python"
      ]
    },
    {
      "id": 181,
      "question": "Что такое `pandas.eval` и чем он полезен?",
      "answer": "`pandas.eval` выполняет выражения над данными с использованием оптимизированного вычислительного движка (`numexpr`), что может ускорять операции на больших DataFrame.\n\n`pandas.eval` полезен для сложных арифметических выражений, но не заменяет стандартный векторизованный API.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Python"
      ]
    },
    {
      "id": 182,
      "question": "Почему важно фиксировать `random seed` в экспериментах?",
      "answer": "Фиксация `random seed` делает результаты экспериментов воспроизводимыми. \n\nЭто позволяет получать одинаковые разбиения `train / test`, повторять результаты bootstrap и симуляций, корректно сравнивать модели и гиперпараметры, упростить отладку и code review.\n    \nБез фиксированного seed небольшие изменения могут приводить к разным результатам, затрудняя анализ.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Python"
      ]
    },
    {
      "id": 183,
      "question": "Почему стоит избегать chained assignment в Pandas?",
      "answer": "Chained assignment — это присваивание, в котором данные выбираются и изменяются в несколько шагов подряд через цепочку индексаций.\n\nChained assignment может привести к неявным ошибкам, так как Pandas не гарантирует, что изменение будет применено к исходному DataFrame.\n\nРекомендуется использовать `loc` для явного присваивания, и `assign` для немутирующих преобразований.\n    \nЕсли не ясно, что именно меняется — значит, код опасен!",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Python"
      ]
    },
    {
      "id": 184,
      "question": "Как реализовать детерминированное сплитование пользователей для эксперимента в Python?",
      "answer": "Детерминированное сплитование — это распределение пользователей по группам так, что один и тот же пользователь всегда попадает в одну и ту же группу. Для этого используют хеширование `user_id` с добавлением соли (идентификатора эксперимента):\n\n```python\ndef bucket(user_id, salt, buckets=100):     \n\treturn hash(f\"{user_id}{salt}\") % buckets\n```\n\nХэш преобразует `user_id` в число, а остаток от деления (`% buckets`) определяет номер бакета. Такое решение обеспечивает стабильное и воспроизводимое распределение без хранения данных в БД.",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "Python"
      ]
    },
    {
      "id": 185,
      "question": "Как быстро оценить MDE в Python?",
      "answer": "MDE (Minimum Detectable Effect) — минимальный эффект, который эксперимент может обнаружить при заданном уровне значимости и размере выборки.\n\nПри нормальном приближении MDE оценивают по формуле: $MDE = z_{α/2} · σ · √(2 / n)$\n\n\n```python\nfrom scipy import stats \nimport math  \n\ndef mde(sigma, n, alpha=0.05):     \n\treturn stats.norm.ppf(1 - alpha / 2) * sigma * math.sqrt(2 / n)\n```\n\nMDE используют для проверки, имеет ли эксперимент смысл при данном размере данных.",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "Python",
        "A/B тесты"
      ]
    },
    {
      "id": 186,
      "question": "Как проверить качество рандомизации с помощью Pandas?",
      "answer": "Качество рандомизации проверяют, сравнивая базовые признаки пользователей между экспериментальными группами.\n\nТиповой подход собрать DataFrame с признаком группы (A/B) и пользовательскими фичами, посчитать агрегаты через `groupby + agg` (mean, std, count), проверить статистические различия. Если различий нет, рандомизация считается корректной.",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "Python"
      ]
    },
    {
      "id": 187,
      "question": "Что делать, если метрика имеет heavy tail и t-тест неустойчив?",
      "answer": "Если распределение метрики имеет длинный хвост и среднее нестабильно, стандартный t-тест может давать некорректные результаты.\n\nПрактические решения - применить лог-преобразование (чтобы уменьшить влияние выбросов), использовать перцентильные метрики (или медиану), перейти на непараметрические методы (bootstrap или тест Mann–Whitney)",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "Статистика",
        "Метрики"
      ]
    },
    {
      "id": 188,
      "question": "Что такое sigmoid и softmax, и в чём их отличия?",
      "answer": "Sigmoid и softmax — это функции, которые переводят «сырые» выходы модели в вероятности, но для разных задач.\n\nSigmoid используется в бинарной классификации. Выдаёт вероятность одного класса — «да или нет». Softmax используется в мультиклассовой классификации. Распределяет вероятность между несколькими классами сразу.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "ML"
      ]
    },
    {
      "id": 189,
      "question": "Что такое One-vs-Rest в машинном обучении?",
      "answer": "One-vs-Rest (OvR, One-vs-All) — это алгоритм классификации, который ищет границу между классами. Для каждого класса обучается отдельный бинарный классификатор; каждый отвечает на вопрос: «это мой класс или нет?»; выбирается класс с наибольшей уверенностью.\n\nПример для 3 классов:\n- модель 1: A vs (B, C)\n- модель 2: B vs (A, C)\n- модель 3: C vs (A, B)  \n    \nOne-vs-Rest часто используют, когда: классов больше двух; данные хорошо разделимы; важна чёткая граница между классами. Силён в задачах с небольшим количеством признаков и хорошо разделимыми классами, но плохо масштабируется на очень большие датасеты.",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "ML"
      ]
    },
    {
      "id": 190,
      "question": "Что такое kNN, где его лучше применять и в чём ограничения?",
      "answer": "kNN (k-Nearest Neighbors) — это простой алгоритм, который не обучается явно, а принимает решение на основе ближайших объектов. «Скажи, кто твои соседи — и я скажу, кто ты». Основная идея: берём объект; находим `k` ближайших соседей; смотрим, каких классов среди них больше.\n    \nХорошо подходит, когда:\n- данных немного\n- простая структура\n- важно быстро получить базовое решение\n    \nОсновные ограничения:\n- медленный на больших датасетах\n- чувствителен к масштабу признаков\n- плохо работает в высоких размерностях\n- требует хранить все данные в памяти",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "ML"
      ]
    },
    {
      "id": 191,
      "question": "Чем отличается дискретная случайная величина от непрерывной?",
      "answer": "Дискретная случайная величина принимает отдельные значения, и каждому из них можно напрямую приписать вероятность. Например, результат броска кубика.\n\nНепрерывная случайная величина принимает значения из непрерывного интервала и описывается функцией плотности. Вероятность попасть ровно в одну точку равна нулю, а вероятности считаются только для интервалов через интеграл.",
      "tags": [],
      "level": "lvl_1",
      "topic": [
        "Теория вероятности"
      ]
    },
    {
      "id": 192,
      "question": "Как посчитать математическое ожидание для дискретной случайной величины?",
      "answer": "Математическое ожидание дискретной случайной величины — это взвешенное среднее её возможных значений, где весами выступают вероятности этих значений. Считается как сумма произведений значений на их вероятности: $E[X] = \\sum x_i \\cdot P(X = x_i)$.\n\nИнтуитивно это «средний результат», к которому будет стремиться случайная величина при большом числе повторений эксперимента. При броске кубика, это будет 3.5",
      "tags": [],
      "level": "lvl_1",
      "topic": [
        "Теория вероятности"
      ]
    },
    {
      "id": 193,
      "question": "Что показывает дисперсия?",
      "answer": "Дисперсия показывает, насколько сильно значения случайной величины в среднем отклоняются от её математического ожидания. Чем больше дисперсия, тем шире разброс значений вокруг среднего; чем меньше - тем значения более сконцентрированы. \n\nФормально дисперсия равна среднему квадрату отклонений от матожидания: $Var(X) = E[(X - E[X])^2]$. Квадрат нужен, чтобы отклонения не уничтожались взаимно и чтобы сильные отклонения наказывались сильнее.",
      "tags": [],
      "level": "lvl_1",
      "topic": [
        "Теория вероятности"
      ]
    },
    {
      "id": 194,
      "question": "Чему равна сумма вероятностей всех возможных исходов дискретной случайной величины?",
      "answer": "Сумма вероятностей всех возможных значений дискретной случайной величины всегда равна 1.",
      "tags": [],
      "level": "lvl_1",
      "topic": [
        "Теория вероятности"
      ]
    },
    {
      "id": 195,
      "question": "Когда применимо распределение Пуассона?",
      "answer": "Распределение Пуассона используется для подсчёта редких событий на фиксированном временном интервале, при независимых событиях, малой вероятности, и большом числе попыток.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Теория вероятности"
      ]
    },
    {
      "id": 196,
      "question": "Опишите биномиальное распределение и его параметры.",
      "answer": "Биномиальное распределение моделирует число успехов в $n$ независимых испытаниях, где в каждом испытании вероятность успеха равна $p$. Обозначается как $X \\sim \\text{Binom}(n, p)$. Математическое ожидание равно $E[X] = np$, дисперсия — $Var(X) = np(1 - p)$.\n\nВ аналитике биномиальное распределение используют для моделирования счётчиков бинарных событий: число конверсий из $n$ показов, количество кликов, число пользователей, совершивших целевое действие.",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "Теория вероятности"
      ]
    },
    {
      "id": 197,
      "question": "Что такое центральная предельная теорема?",
      "answer": "Центральная предельная теорема утверждает, что сумма (или среднее) большого числа независимых одинаково распределённых случайных величин при росте числа наблюдений стремится к нормальному распределению, независимо от формы исходного распределения.\n\nИменно поэтому в аналитике и статистике нормальное распределение так часто возникает при работе с агрегатами и средними значениями.",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "Статистика",
        "Теория вероятности"
      ]
    },
    {
      "id": 198,
      "question": "Что такое функция распределения и каковы её основные свойства?",
      "answer": "Функция распределения случайной величины показывает вероятность того, что она примет значение не больше заданного: $F(x) = P(X \\le x)$. Она неубывающая, непрерывна справа, принимает значения от 0 до 1, при $x \\to -\\infty$ стремится к 0, а при $x \\to +\\infty$ к 1.",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "Теория вероятности"
      ]
    },
    {
      "id": 199,
      "question": "Какое распределение является пределом биномиального при малой $p$ и большом $n$?",
      "answer": "При малой вероятности успеха $p$ и большом числе испытаний $n$ биномиальное распределение стремится к распределению Пуассона. В этом предельном случае параметр распределения равен $\\lambda = np$, а сама модель хорошо описывает редкие события: число ошибок, кликов, сбоев или обращений за фиксированный период.",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "Теория вероятности"
      ]
    },
    {
      "id": 200,
      "question": "Что такое распределение дискретной случайной величины и какие основные виды распределений существуют?",
      "answer": "Распределение дискретной случайной величины описывает, какие значения может принимать величина и с какими вероятностями. По сути, это описание случайного процесса, какое событие мы изучаем и в каком виде.\n\n- Распределение Бернулли — один эксперимент с двумя исходами: успех или неуспех. «Случилось событие или нет?».\n- Биномиальное распределение — число успехов в фиксированном числе независимых испытаний. «Сколько раз произошло событие из n попыток?».\n- Геометрическое распределение — число попыток до первого успеха. «Через сколько шагов событие случится впервые?».\n- Равномерное распределение — все допустимые значения равновероятны. Используется как модель полного отсутствия предпочтений.\n- Распределение Пуассона — число редких событий за фиксированный интервал времени или пространства. «Сколько раз событие произойдёт за период?».\n    \nРазные распределения могут иметь похожие графики при определённых параметрах, но при этом описывать разные процессы. Форма распределения — следствие модели, а не её суть.",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "Теория вероятности"
      ]
    },
    {
      "id": 201,
      "question": "Что такое распределение непрерывной случайной величины и какие основные виды распределений существуют?",
      "answer": "Распределение непрерывной случайной величины описывает, какие значения она может принимать на числовой прямой и как «распределена вероятность» по этим значениям. Для непрерывной величины вероятность попасть ровно в одну точку равна нулю, поэтому вероятности всегда считаются для интервалов.\n\nФункция распределения задаётся так же, как и в общем случае: $F_X(x) = P(X \\le x)$. Она неубывающая, принимает значения от 0 до 1, непрерывна справа, и при $x \\to -\\infty$ стремится к 0, а при $x \\to +\\infty$ к 1. \n\n- Равномерное распределение — «все значения на отрезке одинаково возможны». Удобно как простая модель неопределённости без предпочтений.\n- Экспоненциальное распределение — «время ожидания до события» при постоянной средней интенсивности. Пример: время до следующей ошибки/заявки/звонка. \n- Нормальное распределение — «сумма многих малых независимых факторов». Часто возникает как приближение для средних и сумм (идея ЦПТ). Симметричное распределение вокруг среднего, большие отклонения редки.\n- Стандартное нормальное распределение — это нормальное распределение, приведённое к удобному виду, где $\\mu = 0$, а $\\sigma^2 = 1$. Используется для стандартизации и работы через z-оценки: $Z = \\frac{X - \\mu}{\\sigma}$.\n- Эмпирическая функция распределения — «что реально показывает выборка без предположения о форме распределения». По сути, это доля наблюдений, не превышающих $x$.",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "Теория вероятности"
      ]
    },
    {
      "id": 202,
      "question": "Чему равна вероятность события A при условии события B?",
      "answer": "Условная вероятность события $A$ при условии, что произошло событие $B$, равна доле случаев, в которых происходят оба события, среди всех случаев наступления $B$: $P(A \\mid B) = \\frac{P(A \\cap B)}{P(B)}$, при условии что $P(B) > 0$.",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "Теория вероятности"
      ]
    },
    {
      "id": 203,
      "question": "Что означает независимость событий A и B?",
      "answer": "События $A$ и $B$ называются независимыми, если наступление одного не влияет на вероятность другого. $P(A \\mid B) = P(A)$, или эквивалент $P(A \\cap B) = P(A),P(B)$",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "Теория вероятности"
      ]
    },
    {
      "id": 204,
      "question": "Вероятность дефекта у станка A — 2%, у станка B — 1%. 60% деталей делает станок A. Какова вероятность, что найденный дефект из станка A?\n\nДайте ответ в процентах, округлите значение до целых.",
      "answer": "Это задача на формулу Байеса, мы ищем вероятность того, что дефектная деталь произведена станком A при условии, что дефект уже обнаружен. \n\nОбозначим события:\n- A — деталь произведена станком A\n- B — деталь произведена станком B\n- D — деталь с дефектом\n    \nДано:\n- $P(A) = 0.6$\n- $P(B) = 0.4$\n- $P(D|A) = 0.02$\n- $P(D|B) = 0.01$\n\nНайти нужно:  \n- $P(A|D)$\n\nФормула Байеса + Формула Полной вероятности:\n\n$$P(A|D) =  \\frac{P(D|A) \\cdot P(A)}{P(D)} = \\frac{P(D|A) \\cdot P(A)}{P(D|A) \\cdot P(A) + P(D|B) \\cdot P(B)}$$ \n\n$$P(A|D) = \\frac{0.02 \\cdot 0.6}{0.02 \\cdot 0.6 + 0.01 \\cdot 0.4} = 0.75$$  \n\nОтвет: 75%",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "Теория вероятности"
      ]
    },
    {
      "id": 205,
      "question": "Ренат сменил 4 пары наушников, 3 из которых его устраивали. Теперь он выбирает новую пару наушников.  \n\nОбзор техноблогера: 10 пар наушников проверено, из них 7 хороших.  \n\nРенат проверил модели из рейтинга блогера. Из 8 пар что ему понравились, 5 одобрил блогер. Какова вероятность, что Ренат купит изначально выбранные наушники, если блогер оставил на них положительный отзыв?  \n\nДайте ответ в процентах, округлите значение до целых.",
      "answer": "Определим события:\n- Событие A - {наушники понравились Ренату}  -  $P(A) = \\frac{3}{4}$  - априорная вероятность\n- Событие B - {блогер оценил наушники хорошо} -   $P(B) = \\frac{7}{10}$  - влияющее условие\n- Событие В при условии А - {проверка моделей из списка блогера} - $P(B|A) = \\frac{5}{8}$ \n- Событие A при условии В - {покупка изначально выбранных наушников при положительной оценке блогера} -   $P(A|B)$ - апостериорная вероятность - то что необходимо определить\n\nПо теореме Байеса:\n\n$$\nP(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\n$$\n\n$$\nP(A|B) = \\frac{0.625 \\cdot 0.75}{0.7} \\approx 0.67\n$$  \n\nОтвет: 67%",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "Теория вероятности"
      ]
    },
    {
      "id": 206,
      "question": "Сколькими способами можно выбрать комитет из 3 человек из 10 кандидатов?",
      "answer": "Это число сочетаний  $C_{10}^{3}$\n$$C_n^k = \\frac{n!}{(n-k)! \\cdot k!}$$\n$$C_{10}^{3} = \\frac{10!}{(10-3)! \\cdot 3!} = \\frac{10 \\cdot 9 \\cdot 8}{3 \\cdot 2 \\cdot 1} = 120$$  \n\n\nСуществует 120 способов выбрать комитет из 3 человек из 10 кандидатов.",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "Теория вероятности"
      ]
    },
    {
      "id": 207,
      "question": "Как использовать формулу полной вероятности для каналов привлечения и конверсии?",
      "answer": "Если каналы привлечения $H_i$ образуют полную группу событий, то общая вероятность конверсии считается как взвешенная сумма конверсий по каждому каналу:  \n\n$$P(\\text{конверсия}) = \\sum_i P(\\text{конверсия} \\mid H_i) \\cdot P(H_i)$$\n\nЭто позволяет разложить общую конверсию на вклад отдельных каналов и корректно сравнивать их эффективность без смешивания аудиторий.",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "Теория вероятности"
      ]
    },
    {
      "id": 208,
      "question": "Сформулируйте закон больших чисел.",
      "answer": "Среднее независимых одинаково распределённых величин сходится к математическому ожиданию при росте числа наблюдений.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Теория вероятности"
      ]
    },
    {
      "id": 209,
      "question": "Что такое условная вероятность и как её вычислить?",
      "answer": "Условная вероятность характеризует вероятность события A при условии, что событие B произошло. $$P(A|B) = \\frac{P(A ∩ B)}{P(B)}$$",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Теория вероятности"
      ]
    },
    {
      "id": 210,
      "question": "Объясните формулу полной вероятности.",
      "answer": "Формула полной вероятности используется для нахождения общей вероятности события через сумму вероятностей условных событий, покрывающих всё пространство.\n\nЕсли $B_n$ – полная группа событий, тогда: $P(A) = \\sum P(A|B_i) \\cdot P(B_i)$\n\n$B$ и $\\bar{B}$ всегда образуют полную группу событий, поэтому: $P(A) = P(A|B) \\cdot P(B) + P(A|\\bar{B}) \\cdot P(\\bar{B})$",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Теория вероятности"
      ]
    },
    {
      "id": 211,
      "question": "Что такое теорема Байеса и когда её использовать?",
      "answer": "Теорема Байеса – это правило, по которому априорная вероятность превращается в апостериорную. Позволяет обновлять вероятности гипотез при получении новых данных. \n\n$$\\quad P(A \\ | \\ B) = \\frac{P(B \\ | \\ A) \\ \\cdot \\ P(A)}{P(B)}$$",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Теория вероятности"
      ]
    },
    {
      "id": 212,
      "question": "В почтовом ящике Макса 100 писем, из них 20 писем это спам.  \n21% спам сообщений содержат слово «лотерея».  \n2% не спам сообщений также содержит слово  «лотерея».    \n\nМакс получает новое письмо со словом «лотерея».  \nКакова вероятность, что это письмо - спам?  \n\nДайте ответ в процентах, округлите значение до целых.",
      "answer": "Определение событий:  \n- $S$ – письмо является спамом - априорная вероятность \n- $W$ – письмо содержит слово «лотерея» - событие влияющее на априорную вероятность\n\nУсловия:  \n- $P(S) = 0.2$  \n- $P(\\bar{S}) = 0.8$  \n- $P(W|S) = 0.21$  \n- $P(W|\\bar{S}) = 0.02$    \n\nНайти:  \n- $P(S|W)$ - апостериорная вероятность того, что новое сообщение со словом \"лотерея\" окажется спамом\n \nФормула полной вероятности:\n\n$$\nP(W) = P(W|S) \\cdot P(S) + P(W|\\bar{S}) \\cdot P(\\bar{S})\n$$\n\n$$\nP(W) = 0.21 \\cdot 0.2 + 0.02 \\cdot 0.8 = 0.058\n$$\n\nФормула Байеса:\n\n$$\nP(S|W) = \\frac{P(W|S) \\cdot P(S)}{P(W)}\n$$\n\n$$\nP(S|W) = \\frac{0.21 \\cdot 0.2}{0.058} \\approx 0.72\n$$\n\nОтвет: 72%",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "Теория вероятности"
      ]
    },
    {
      "id": 213,
      "question": "Как связаны независимость событий и произведение вероятностей?",
      "answer": "События независимы тогда и только тогда, когда $P(A ∩ B) = P(A) \\cdot P(B)$  \nЕсли $P(A ∩ B) \\ne P(A) \\cdot P(B)$, значит события зависимы.  \n\n\n«Если одно событие влияет на другое, значит они зависимы» - не верное предположение! Влияние формализуется через вероятности, а не «интуитивно».",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Теория вероятности"
      ]
    },
    {
      "id": 214,
      "question": "Как посчитать вероятность того, что произойдёт хотя бы одно из событий?",
      "answer": "Использовать принцип включений–исключений: складываем вероятности событий и вычитаем вероятности их пересечений.\n\nДля двух событий:  $P(A \\cup B) = P(A) + P(B) - P(A \\cap B)$",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Теория вероятности"
      ]
    },
    {
      "id": 215,
      "question": "Сформулируйте закон полного математического ожидания",
      "answer": "Закон полного математического ожидания — если пространство исходов разбито на взаимоисключающие случаи $B_i$ (покрывают все исходы и не пересекаются), то общее математическое ожидание это взвешенная сумма условных ожиданий по этим случаям. \n\n$$\\mathbb{E}[X] = \\sum_i \\mathbb{E}[X \\mid B_i] \\ P(B_i)$$\n\n Считаем среднее «по сегментам» и усредняем их с весами вероятностей сегментов.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Теория вероятности"
      ]
    },
    {
      "id": 216,
      "question": "Каков ожидаемый результат подбрасывания честной монеты 10 раз по числу орлов?",
      "answer": "Математическое ожидание биномиального распределения $np = 10 \\cdot 0.5 = 5$  \nОжидаемый результат 5 орлов.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Теория вероятности"
      ]
    },
    {
      "id": 217,
      "question": "Как вычислить математическое ожидание непрерывной случайной величины?",
      "answer": "Для непрерывной случайной величины математическое ожидание считается через интеграл произведения значения $x$ на плотность распределения $f(x)$  \n  \n$$\\mathbb{E}[X] = \\int_{-\\infty}^{\\infty} x \\cdot f(x) \\ dx$$",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Теория вероятности"
      ]
    },
    {
      "id": 218,
      "question": "Как интерпретировать дисперсию случайной величины?",
      "answer": "Дисперсия случайной величины показывает, насколько сильно значения случайной величины в среднем разбросаны вокруг математического ожидания.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Теория вероятности"
      ]
    },
    {
      "id": 219,
      "question": "Что описывает функция распределения $F(x)$?",
      "answer": "Функция распределения показывает вероятность того, что случайная величина не превышает значение x, или $F(x) = P(X ≤ x)$",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Теория вероятности"
      ]
    },
    {
      "id": 220,
      "question": "Сформулируйте теорему Байеса и поясните, когда её применять.",
      "answer": "Теорема Байеса – это правило, по которому априорная вероятность превращается в апостериорную. Применяется, когда нужно обновить вероятность гипотезы после наблюдения события A. \n\n$$P(A \\ | \\ B) = \\frac{P(B \\ | \\ A) \\ \\cdot \\ P(A)}{P(B)}$$",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Теория вероятности"
      ]
    },
    {
      "id": 221,
      "question": "Приведите пример разбиения пространства для применения теоремы полной вероятности.",
      "answer": "Например, пользователей разбивают по каналам привлечения (поисковый трафик, реклама, рекомендации). Общую вероятность конверсии считают как сумму вероятностей конверсии в каждом канале, взвешенных на долю пользователей этого канала.\n\n$$P(\\text{конверсия}) = \\sum_i P(\\text{конверсия} \\mid \\text{канал}_i) \\cdot  P(\\text{канал}_i)$$",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Теория вероятности"
      ]
    },
    {
      "id": 222,
      "question": "Как найти вероятность 2 успехов в схеме Бернулли при n=5, p=0.2?",
      "answer": "Если проводится n независимых испытаний с вероятностью успеха p, то вероятность k успехов равна:  $P(X=k) = C_n^k \\cdot p^k \\cdot (1-p)^{n-k} = C_5^2 \\cdot 0.2^2 \\cdot 0.8^3 = 0.2048$\n\nВероятность двух успехов равна 20%",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Теория вероятности"
      ]
    },
    {
      "id": 223,
      "question": "Что такое распределение Бернулли?",
      "answer": "Распределение Бернулли описывает один случайный эксперимент с двумя исходами: успех 1 и неудача 0 с вероятностью успеха p. \n\nОбозначается:  $\\quad X \\sim \\mathrm{Bern}(p)$   \nДисперсия:  $\\quad \\mathrm{Var}(X) = p \\ (1-p)$  \nМатематическое ожидание:  $\\quad \\mathbb{E}[X] = p$",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Теория вероятности"
      ]
    },
    {
      "id": 224,
      "question": "Что такое геометрическое распределение?",
      "answer": "Геометрическое распределение описывает число испытаний до первого успеха в серии независимых испытаний Бернулли с вероятностью успеха p.\n\nОбозначается:  $\\quad X \\sim \\mathrm{Geom}(p)$   \nДисперсия:  $\\quad \\text{Var}[X] = \\frac{1 - p}{p^2}$   \nМатематическое ожидание: $\\quad \\mathbb{E}[X] = \\frac{1}{p}$",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Теория вероятности"
      ]
    },
    {
      "id": 225,
      "question": "Что такое распределение Пуассона?",
      "answer": "Распределение Пуассона описывает число событий, происходящих за фиксированный интервал времени или пространства при известной средней интенсивности $\\lambda$.\n\nОбозначается:  $\\quad X \\sim \\mathrm{Pois}(\\lambda)$    \nДисперсия:  $\\quad \\mathrm{Var}(X) = \\lambda$  \nМатематическое ожидание:  $\\quad \\mathbb{E}[X] = \\lambda$",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Теория вероятности"
      ]
    },
    {
      "id": 226,
      "question": "Что такое равномерное распределение?",
      "answer": "Равномерное распределение — все возможные значения принимаются с одинаковой вероятностью. Например: честный кубик.\n\nОбозначается: $\\quad  X \\sim {U}(a, b)$  \nФункция вероятности:  $\\quad  p(x) = \\frac{1}{n}$     \nМатематическое ожидание:  $\\quad  \\mathbb{E}[X] = \\frac{a + b}{2}$",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Теория вероятности"
      ]
    },
    {
      "id": 227,
      "question": "Что такое Биномиальное распределение?",
      "answer": "Биномиальное распределение описывает вероятность $k$ успехов в $n$ независимых испытаниях Бернулли с вероятностью успеха $p$ в каждом. \n\nОбозначается: $\\quad  X \\sim \\text{Binom}(p, n)$  \nДисперсия:  $\\quad \\text{Var}[X] = np \\ (1 - p)$  \nФункция вероятности:  $\\quad p(k) = C_n^k \\cdot p^k \\cdot (1 - p)^{n - k}$  \nМатематическое ожидание:  $\\quad E[X] = np$",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Теория вероятности"
      ]
    },
    {
      "id": 228,
      "question": "Сформулируйте закон больших чисел.",
      "answer": "Среднее независимых одинаково распределённых величин сходится к ожиданию при $n→∞$.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Теория вероятности"
      ]
    },
    {
      "id": 229,
      "question": "Приведите пример применения закона больших чисел в продуктовой аналитике.",
      "answer": "При большом числе заказов выборочное среднее среднего чека стабилизируется и приближается к истинному среднему, поэтому метрику можно надёжно оценивать по данным.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Теория вероятности"
      ]
    },
    {
      "id": 230,
      "question": "Что такое нормальное распределение?",
      "answer": "Нормальное распределение описывает непрерывную случайную величину, значения которой симметрично распределены вокруг среднего $\\mu$ с разбросом $\\sigma$. Особенность нормального распределения в том, что его параметры — это характеристики, а характеристики — параметры.\n\nОбозначается: $\\quad X \\sim {N}(\\mu, \\sigma^2)$  \nМатематическое ожидание: $\\quad \\mathbb{E}[X] = \\mu$  \nДисперсия: $\\quad \\mathrm{Var}(X) = \\sigma^2$",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Теория вероятности"
      ]
    },
    {
      "id": 231,
      "question": "Что такое стандартное нормальное распределение?",
      "answer": "Стандартное нормальное распределение — это нормальное распределение с нулевым средним и единичной дисперсией. Используют в случаях, когда нужно избавиться от размерности исходной случайной величины.\n\nОбозначается: $\\quad X \\sim {Z}(0, 1)$  \nМатематическое ожидание: $\\quad \\mathbb{E}[X] = 0$  \nДисперсия: $\\quad \\mathrm{Var}(X) = 1$",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Теория вероятности"
      ]
    },
    {
      "id": 232,
      "question": "Что такое эмпирическое распределение?",
      "answer": "Эмпирическое распределение — распределение, построенное непосредственно по наблюдаемым данным без предположений о виде теоретического распределения.\n\nМатематическое ожидание и дисперсия вычисляются по выборке.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Теория вероятности"
      ]
    },
    {
      "id": 233,
      "question": "Сформулируйте центральную предельную теорему.",
      "answer": "Сумма (или среднее) большого числа независимых случайных величин стремится к нормальному распределению независимо от исходного распределения.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Теория вероятности"
      ]
    },
    {
      "id": 234,
      "question": "Объясните на примере, почему ЦПТ позволяет строить доверительные интервалы.",
      "answer": "По Центральной предельной теореме при большом объёме данных выборочное среднее становится близким к нормальному распределению, поэтому его разброс можно оценить и построить доверительный интервал с использованием z-распределения.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Теория вероятности"
      ]
    },
    {
      "id": 235,
      "question": "Что такое p-value в A/B тестах?",
      "answer": "P-value в A/B тестах это вероятность получить столь же экстремальную статистику при условии верности $H_0$.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Теория вероятности",
        "A/B тесты"
      ]
    },
    {
      "id": 236,
      "question": "Есть некоторая случайная величина нормально распределённая с $μ=5$ и $σ=1$.  \nКак рассчитать вероятность получить значение 6 или более в Python?",
      "answer": "Используем библиотеку `scipy`. Метод `norm` создает случайную величину нормального распределения, метод `cdf` вычисляет вероятность в точке. `loc` определяет математическое ожидание *μ*, `scale` определяет значение стандартного отклонения *σ*.\n\n```python\nfrom scipy.stats import norm\n\nmu = 5     \nsigma = 1  \nx = 6      \n\nX = norm(loc = mu, scale = sigma) \nresult = X.cdf(x=x) \n\nprint(result)  # 0.8413447460685429\n```",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Python",
        "Теория вероятности"
      ]
    },
    {
      "id": 237,
      "question": "Есть некоторое среднее время, от начала до некоторого события, равное 45 секунд. \nНадо найти вероятность совершения события спустя 2 минуты. Как решить данную задачу в Python?",
      "answer": "Имеем случайную величину Экспоненциально распределенную. Надо определить параметр λ и точку, в которой будем считать значение вероятности. Используем библиотеку `scipy`. Метод `cdf` вычисляет вероятность в точке. Функция `expon` в качестве параметра использует не сам параметр λ, а матожидание ($E[X]=1/λ$), поэтому необходимо добавить новую переменную `E`.\n\n```python\nfrom scipy.stats import expon\n\nLambda = 1 / 45\nE = 1 / Lambda\nx = 120 \n\n# вероятность правее точки x\np = 1 - expon.cdf(x=x, scale=E)  \n\nprint(p)  # 0.06948345122280153\n```",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Python",
        "Теория вероятности"
      ]
    },
    {
      "id": 238,
      "question": "Мы ждем звонка с 13:25 до 15:05. С какой вероятностью нам позвонят в первые 30 минут описанного временного промежутка, если известно что звонок происходит в случайный момент? Решите задачу в Python.",
      "answer": "Имеем случайную величину непрерывного равномерного распределения. Используем библиотеку `scipy`. Метод `cdf` вычисляет вероятность в точке. Параметр $a$ отвечает за начало распределения, параметр $b$ за конец  распределения (точка, в которой функция начинает быть равна 1).\n\n```python \nfrom scipy.stats import uniform\n\na = 0\nb = 100  # 15:05 - 13:25 = 100 \nx = 30  \n\np = uniform.cdf(x=x, loc=a, scale=b)\n\nprint(p)  # 0.3\n```",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Python",
        "Теория вероятности"
      ]
    },
    {
      "id": 239,
      "question": "Чем отличаются выборка и генеральная совокупность?",
      "answer": "Генеральная совокупность — все возможные наблюдения, выборка — их часть, по которой оценивают параметры совокупности.",
      "tags": [],
      "level": "lvl_1",
      "topic": [
        "Статистика"
      ]
    },
    {
      "id": 240,
      "question": "Что такое медиана и чем она полезна?",
      "answer": "Медиана — центральное значение упорядоченных данных, устойчива к выбросам в отличие от среднего.",
      "tags": [],
      "level": "lvl_1",
      "topic": [
        "Статистика"
      ]
    },
    {
      "id": 241,
      "question": "Когда применять критерий Манна–Уитни вместо t-теста?",
      "answer": "Когда предпосылки t-теста нарушены: выборки малые, распределения далеки от нормальных или есть выбросы — Манна–Уитни сравнивает распределения (часто медианы) и не требует нормальности.",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "Статистика"
      ]
    },
    {
      "id": 242,
      "question": "Что такое t-тест?",
      "answer": "t-тест — параметрический тест для проверки гипотезы о равенстве средних двух выборок при предположении нормальности распределений.",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "Статистика"
      ]
    },
    {
      "id": 243,
      "question": "В чём отличие параметрических и непараметрических тестов?",
      "answer": "Параметрические тесты предполагают конкретный вид распределения данных и проверяют его параметры (например, среднее), а непараметрические не предполагают форму распределения и основаны на рангах или эмпирическом распределении данных.",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "Статистика"
      ]
    },
    {
      "id": 244,
      "question": "Что такое параметрические тесты в статистике?",
      "answer": "Параметрические тесты — это статистические методы, которые предполагают конкретную форму распределения данных и проверяют гипотезы о его параметрах, например о среднем или дисперсии.",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "Статистика"
      ]
    },
    {
      "id": 245,
      "question": "Что такое непараметрические тесты в статистике?",
      "answer": "Непараметрические тесты — это статистические методы, которые не предполагают конкретный вид распределения данных и делают выводы на основе рангов или эмпирического распределения наблюдений.",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "Статистика"
      ]
    },
    {
      "id": 246,
      "question": "Какие виды параметрических тестов бывают?",
      "answer": "Основные виды параметрических тестов:\n- t-тесты — проверка средних (одновыборочный, для независимых выборок, парный)\n- z-тест — проверка среднего при известной дисперсии или больших выборках\n- ANOVA — сравнение средних более чем в двух группах\n- Регрессионные тесты — t- и F-тесты значимости коэффициентов в линейной регрессии",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "Статистика"
      ]
    },
    {
      "id": 247,
      "question": "Какие виды непараметрических тестов бывают?",
      "answer": "Основные виды непараметрических тестов:\n- Манна–Уитни — сравнение двух независимых выборок\n- Уилкоксона — сравнение двух связанных (парных) выборок\n- Краскела–Уоллиса — сравнение более чем двух независимых выборок\n- Колмогорова–Смирнова — сравнение распределений\n- χ² (хи-квадрат) — анализ категориальных данных и проверка независимости",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "Статистика"
      ]
    },
    {
      "id": 248,
      "question": "Что такое бутстрэп и зачем он нужен?",
      "answer": "Бутстрэп — это симуляция повторного сбора данных. Нужен, чтобы оценить неопределённость статистики по данным, когда нет аналитических формул или нельзя полагаться на нормальность распределения.",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "Статистика"
      ]
    },
    {
      "id": 249,
      "question": "Что такое ANOVA?",
      "answer": "ANOVA (Analysis of Variance) — это параметрический тест, который проверяет равны ли средние значения в нескольких группах (2 и более), анализируя за счёт чего возникает разброс данных: случайного шума ВНУТРИ групп, или различий МЕЖДУ группами.\n\nГипотеза $H_0: \\ \\mu_1 = \\mu_2 = \\dots = \\mu_n$   \nГипотеза $H_1: \\ \\exists, i,j : \\mu_i \\ne \\mu_j$  \n\nЕсли разброс между группами значительно больше, чем внутри групп, значит средние отличаются не случайно. Тест ANOVA не говорит, какие именно группы различаются, только что различие есть.",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "Статистика"
      ]
    },
    {
      "id": 250,
      "question": "Чем ANOVA отличается от множества t-тестов?",
      "answer": "ANOVA проверяет различие средних во всех группах сразу и контролирует общий уровень значимости, тогда как множество t-тестов увеличивает вероятность ошибки I рода.\n\n- много t-тестов → растёт шанс случайно найти «значимый» эффект\n- ANOVA → один глобальный тест, один фиксированный  α",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "Статистика"
      ]
    },
    {
      "id": 251,
      "question": "Что такое множественная проверка гипотез и как контролируют FDR?",
      "answer": "При проверке множества гипотез возрастает вероятность ложноположительных выводов. Для контроля ошибок используют методы множественных сравнений: Bonferroni контролирует FWER — вероятность хотя бы одной ошибки, а Benjamini–Hochberg контролирует FDR — долю ложных находок среди всех отклонённых гипотез.",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "Статистика"
      ]
    },
    {
      "id": 252,
      "question": "Что такое FWER (Family-Wise Error Rate)?",
      "answer": "FWER — это вероятность допустить хотя бы одну ошибку I рода при множественной проверке гипотез.",
      "tags": [],
      "level": "lvl_1",
      "topic": [
        "Статистика"
      ]
    },
    {
      "id": 253,
      "question": "Что такое FDR (False Discovery Rate)?",
      "answer": "FDR — это доля ложных срабатываний среди всех отклонённых гипотез при множественном тестировании.",
      "tags": [],
      "level": "lvl_1",
      "topic": [
        "Статистика"
      ]
    },
    {
      "id": 254,
      "question": "Что такое поправка Бонферрони?",
      "answer": "Поправка Бонферрони — это метод контроля FWER, при котором уровень значимости делят на число тестов: $\\alpha_{\\text{new}} = \\frac{\\alpha}{m}​$. Метод строгий и снижает мощность тестов.",
      "tags": [],
      "level": "lvl_1",
      "topic": [
        "Статистика"
      ]
    },
    {
      "id": 255,
      "question": "Что такое метод Benjamini–Hochberg?",
      "answer": "Benjamini–Hochberg — метод контроля FDR, который допускает небольшую долю ложных находок ради большей мощности и используется при большом числе тестов.",
      "tags": [],
      "level": "lvl_1",
      "topic": [
        "Статистика"
      ]
    },
    {
      "id": 256,
      "question": "Что такое байесовский интервал достоверности?",
      "answer": "Байесовский интервал достоверности — это диапазон значений параметра, в котором он находится с заданной вероятностью согласно апостериорному распределению, полученному из данных и априорных предположений.",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "Статистика"
      ]
    },
    {
      "id": 257,
      "question": "Как по-разному интерпретируются 95% частотный и байесовский интервалы?",
      "answer": "95% частотный интервал означает, что метод в 95% повторений накрывает истинный параметр, а 95% байесовский интервал означает, что параметр находится в этом диапазоне с вероятностью 95%.",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "Статистика"
      ]
    },
    {
      "id": 258,
      "question": "В чём основное различие между частотным доверительным интервалом и байесовским доверительным интервалом?",
      "answer": "Классический (частотный) доверительный интервал — при многократном повторении эксперимента построенные интервалы накрывают истинный параметр. Здесь параметр считается фиксированным, интервал - случайным. \n\nБайесовский доверительный интервал — описывает неопределённость параметра и показывает, с какой вероятностью он лежит в данном диапазоне при условии априорных предположений. Здесь параметр считается случайной величиной, а интервал - утверждением о параметре.",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "Статистика"
      ]
    },
    {
      "id": 259,
      "question": "Как контролировать суммарную вероятность ложных срабатываний при множественных тестах?",
      "answer": "Применить поправку Бонферрони. Поправка Бонферрони (или FDR) уменьшает вероятность ошибки I рода при множестве проверок.",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "Статистика"
      ]
    },
    {
      "id": 260,
      "question": "Какой статистический тест используют для проверки нормальности малых выборок?",
      "answer": "Для проверки нормальности малых выборок используют тест Шапиро–Уилка, так как он обладает высокой чувствительностью к отклонениям от нормальности при небольшом размере выборки (n<50). Тест применим и при больших n, но для малых считается наиболее эффективным.",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "Статистика"
      ]
    },
    {
      "id": 261,
      "question": "Почему статистические тесты на нормальность плохо работают при больших выборках?",
      "answer": "При больших выборках тесты на нормальность становятся слишком чувствительными и обнаруживают статистически значимые, но практически несущественные отклонения от нормальности.",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "Статистика"
      ]
    },
    {
      "id": 262,
      "question": "Как на практике проверяют нормальность при больших выборках?",
      "answer": "При больших выборках нормальность оценивают визуально (Q–Q plot), по наличию выбросов и асимметрии, а также по робастности методов, полагаясь на ЦПТ, а не на формальные тесты.",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "Статистика"
      ]
    },
    {
      "id": 263,
      "question": "Что такое робастность?",
      "answer": "Статистический метод предполагает нормальность (или другие условия), чтобы его выводы были строго корректны, но он может также сохранять корректность выводов при умеренных нарушениях этих предположений. Эта устойчивость и есть робастность.\n\nПри больших выборках, t-тест часто применяют даже при ненормальных данных. Поэтому говорят, t-тест робастен к умеренной ненормальности.",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "Статистика"
      ]
    },
    {
      "id": 264,
      "question": "Что такое критерий Манна–Уитни?",
      "answer": "Критерий Манна–Уитни — непараметрический тест для сравнения двух независимых выборок, основанный на рангах и не требующий нормальности распределений.",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "Статистика"
      ]
    },
    {
      "id": 265,
      "question": "Поясните, что такое доверительный интервал для среднего.",
      "answer": "Доверительный интервал для среднего — это интервал, построенный по выборке, который при многократном повторении эксперимента с заданной вероятностью накрывает истинное среднее; обычно имеет вид  $\\bar x \\pm z \\cdot \\frac{\\sigma}{\\sqrt{n}}$  (при нормальности или большом объёме выборки).",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Статистика"
      ]
    },
    {
      "id": 266,
      "question": "Что такое p-value и как его интерпретировать?",
      "answer": "p-value  это вероятность получить наблюдаемое или более экстремальное значение статистики при условии, что нулевая гипотеза верна; чем меньше p-value, тем меньше согласие данных с $H_0$​ и тем больше оснований её отвергнуть.\n\nНулевую гипотезу отвергают, если p-value меньше заранее выбранного уровня значимости $\\alpha$.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Статистика",
        "A/B тесты"
      ]
    },
    {
      "id": 267,
      "question": "В чём разница между ошибками I и II рода?",
      "answer": "Ошибка I рода — отклонение истинной нулевой гипотезы (ложно положительное срабатывание; уровень значимости $\\alpha$), ошибка II рода — не отклонение ложной нулевой гипотезы (ложно отрицательное срабатывание; вероятность $\\beta$).",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Статистика",
        "A/B тесты"
      ]
    },
    {
      "id": 268,
      "question": "Как проверить нормальность распределения?",
      "answer": "Нормальность проверяют визуально с помощью Q–Q plot, формальными тестами (Шапиро–Уилка, Колмогорова–Смирнова), а также по асимметрии и эксцессу.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Статистика"
      ]
    },
    {
      "id": 269,
      "question": "Как рассчитывается корреляция Пирсона и когда она подходит?",
      "answer": "Корреляция Пирсона измеряет силу и направление линейной связи между двумя количественными переменными и подходит при линейной зависимости, отсутствии сильных выбросов и примерно симметричных распределениях.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Статистика"
      ]
    },
    {
      "id": 270,
      "question": "Что такое стандартная ошибка среднего?",
      "answer": "Если известно стандартное отклонение генеральной совокупности $\\sigma$, то стандартная ошибка среднего равна:\n\n$$  \nSE = \\frac{\\sigma}{\\sqrt{n}}  \n$$\n\nСтандартная ошибка характеризует разброс оценки среднего значения при повторных выборках из генеральной совокупности. На практике $\\sigma$ обычно неизвестна и заменяется на выборочное стандартное отклонение $s$.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Статистика"
      ]
    },
    {
      "id": 271,
      "question": "Что такое Sample Ratio Mismatch и как его обнаружить?",
      "answer": "Sample Ratio Mismatch — это отклонение фактических долей пользователей в группах A/B-теста от запланированных (планировали 50% в A и 50% в B, а фактически получили 60% / 40%), обычно из-за ошибок рандомизации или трекинга; обнаруживают сравнением долей и χ²-тестом. При наличии SRM результатам теста доверять нельзя, потому что группы стали несопоставимы. Эксперимент считают некорректным.",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "Статистика",
        "A/B тесты"
      ]
    },
    {
      "id": 272,
      "question": "Что показывает Boxplot?",
      "answer": "Boxplot визуализирует распределение данных: медиану, квартильный размах (Q1–Q3), усы и потенциальные выбросы.",
      "tags": [],
      "level": "lvl_1",
      "topic": [
        "Статистика",
        "Метрики"
      ]
    },
    {
      "id": 273,
      "question": "Какой критерий используют для проверки равенства средних двух нормальных выборок при неизвестных, но равных дисперсиях?",
      "answer": "Используют t-тест Стьюдента, который предназначен для сравнения средних двух нормальных выборок при неизвестных, но предполагаемо равных дисперсиях.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Статистика"
      ]
    },
    {
      "id": 274,
      "question": "Как интерпретировать коэффициент корреляции Пирсона 0.8?",
      "answer": "Коэффициент Пирсона 0.8 означает высокую положительную линейную зависимость признаков.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Статистика"
      ]
    },
    {
      "id": 275,
      "question": "Что показывает AUC ROC?",
      "answer": "AUC ROC (Area Under Curve - Receiver Operating Characteristic) показывает вероятность того, что модель присвоит положительному объекту больший скор, чем отрицательному, и отражает её способность корректно ранжировать объекты.\n- AUC = 0.5 → случайное угадывание\n- AUC = 1.0 → идеальное ранжирование\n- не зависит от выбранного порога классификации",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Статистика"
      ]
    },
    {
      "id": 276,
      "question": "Что измеряет коэффициент детерминации $R^2$?",
      "answer": "Коэффициент детерминации $R^2$ показывает долю вариации целевой переменной, объяснённую моделью, по сравнению с предсказанием среднего.\n\n$$R^2 = 1 - \\frac{SSE}{SST}$$  \n\nSST — общий разброс данных (если предсказывать только среднее)  \nSSE — остаточный разброс после модели\n\n- модель идеальна → SSE = 0 → ($R^2 = 1$)\n- модель не лучше среднего → SSE = SST → ($R^2 = 0$)\n- модель хуже среднего → SSE > SST → ($R^2 < 0$)\n\nR² показывает насколько модель лучше среднего.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Статистика"
      ]
    },
    {
      "id": 277,
      "question": "Когда предпочтительнее критерий Манна-Уитни?",
      "answer": "Критерий Манна–Уитни используют при малых выборках, ненормальных распределениях или наличии выбросов, когда предпосылки t-теста нарушены. Манна-Уитни это непараметрический тест, сравнивающий распределения на основе рангов.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Статистика"
      ]
    },
    {
      "id": 278,
      "question": "Какой график лучше всего показывает доли целого?",
      "answer": "Доли целого обычно показывают круговой диаграммой (pie chart), но при большом числе категорий или для точных сравнений предпочтительнее столбчатая диаграмма (bar chart).",
      "tags": [],
      "level": "lvl_1",
      "topic": [
        "Метрики"
      ]
    },
    {
      "id": 279,
      "question": "Какой тип графика лучше использовать для сравнения распределений нескольких категорий?",
      "answer": "Для сравнения распределений нескольких категорий хорошо подходит **violin plot**, так как он показывает форму распределения (плотность), медиану и разброс данных.",
      "tags": [],
      "level": "lvl_1",
      "topic": [
        "Метрики"
      ]
    },
    {
      "id": 280,
      "question": "Чем $R^2$ отличается от MSE / RMSE?",
      "answer": "MSE/RMSE измеряют абсолютную ошибку прогнозов (RMSE — в единицах целевой), а $R^2$ показывает относительное качество: насколько модель лучше базлайна “предсказывать среднее” (доля объяснённой вариации).",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Статистика"
      ]
    },
    {
      "id": 281,
      "question": "В чём идея последовательного теста?",
      "answer": "Последовательный тест позволяет анализировать результаты по мере поступления данных и останавливать эксперимент раньше, при этом контролируя ошибки (например, через границы остановки или α-расход), чтобы многократные проверки не раздували ложноположительные результаты.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Статистика"
      ]
    },
    {
      "id": 282,
      "question": "Какие условия нужны для t-теста Стьюдента?",
      "answer": "t-тест сравнивает средние и требует независимости наблюдений, (примерной) нормальности метрики или достаточно большого nnn, а для классической версии — близких дисперсий; при разных дисперсиях используют t-тест Уэлча.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Статистика"
      ]
    },
    {
      "id": 283,
      "question": "Как построить 95% доверительный интервал для среднего времени сессии  \n$(\\bar{x}=12), (\\sigma=4), (n=100)$?",
      "answer": "Так как $\\sigma$ известна, используем z-интервал:  \n$$CI = \\bar{x} \\pm z_{0.975} \\cdot SE$$  \n\nСтандартная ошибка:  \n$$SE = \\frac{\\sigma}{\\sqrt{n}} = \\frac{4}{\\sqrt{100}} = 0.4$$  \n\n\nПогрешность:  \n$$ME=z_{0.975} \\cdot SE = 1.96 \\cdot 0.4 = 0.784$$  \n\nИтог:  \n$$CI = 12 \\pm 0.784 = [11.22;\\ 12.78]$$  \n\nПри повторении эксперимента 95% таких интервалов накрывали бы истинное среднее время сессии.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Статистика"
      ]
    },
    {
      "id": 284,
      "question": "Как построить 95% доверительный интервал для среднего времени сессии  \n$(\\bar{x}=12), (s=4), (n=100)$?",
      "answer": "Так как $\\sigma$ неизвестна, используем t-интервал:  \n\n$$CI = \\bar{x} \\pm t_{0.975,df} \\cdot SE$$\n\nСтандартная ошибка для t-интервала:  \n$$SE = \\frac{s}{\\sqrt{n}} = \\frac{4}{\\sqrt{100}} = 0.4$$  \n\nОценка для 95% t-интервала:  \n$$df = n - 1 = 99$$  \n$$t_{0.975,99} \\approx 1.984$$  \n\nПогрешность:  \n$$ME = t_{0.975,99} \\cdot SE = 1.984 \\cdot 0.4 = 0.794$$  \n\nИтог:  \n$$CI = 12 \\pm 1.984\\cdot 0.4 = [11.21;\\ 12.79]$$",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Статистика"
      ]
    },
    {
      "id": 285,
      "question": "Как рассчитать t-статистику для разницы средних $(\\bar{x}_a=120), (\\bar{x}_b=125), (s=30), (n_a=n_b=400)$?",
      "answer": "Гипотезы: \n$$H_0 : \\bar{x}_a = \\bar{x}_b$$\n$$H_1 : \\bar{x}_a \\ne \\bar{x}_b$$\n\nРазница средних:  \n$$\\Delta = 125-120=5$$  \n\nСтандартная ошибка:  \n$$SE = s \\cdot \\sqrt{\\frac{1}{n_a}+\\frac{1}{n_b}}  \n=30 \\cdot \\sqrt{\\frac{2}{400}}  \n\\approx 2.121$$   \n\nt-статистика:  \n$$t=\\frac{\\Delta}{SE}=\\frac{5}{2.121}\\approx 2.36$$  \n\nВывод:  \n\nt-критическое значение при $\\alpha=0.05$ (двустороннем тесте) равно 1.96\n$$|t| \\approx 2.36 > 1.96$$    \nНулевая гипотеза о равенстве средних отвергается. Средние выборок статистически значимо различаются.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Статистика"
      ]
    },
    {
      "id": 286,
      "question": "Чем отличается односторонний и двусторонний тест и когда выбирать каждый?",
      "answer": "Двусторонний тест проверяет любое отличие $(H_1:\\ \\mu_B \\ne \\mu_A)$ и используется по умолчанию, когда важны как улучшения, так и ухудшения. Односторонний тест проверяет эффект в заданную сторону $(H_1:\\ \\mu_B > \\mu_A)$, его выбирают только если направление эффекта заранее обосновано и ухудшение не является релевантным исходом.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Статистика"
      ]
    },
    {
      "id": 287,
      "question": "Что такое дисперсия?",
      "answer": "Дисперсия — это мера разброса данных, показывающая, насколько значения в среднем отклоняются от среднего. \n\nЕсли дисперсия маленькая, значения «скучены» около среднего; если большая — данные сильно разбросаны. Дисперсия измеряется в квадрате единиц исходной величины, поэтому напрямую интерпретировать её неудобно.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Статистика"
      ]
    },
    {
      "id": 288,
      "question": "Чем отличается выборочная и генеральная дисперсия?",
      "answer": "Выборочная дисперсия $s^2$ оценивает разброс по выборке и делится на (n-1), а дисперсия генеральной совокупности $\\sigma^2$ — истинный параметр распределения и делится на N.\n\nДеление на (n-1) — это поправка Бесселя: она компенсирует смещение, возникающее из-за оценки среднего по тем же данным, и делает оценку дисперсии несмещённой.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Статистика"
      ]
    },
    {
      "id": 289,
      "question": "Что такое стандартное отклонение?",
      "answer": "Стандартное отклонение — это корень из дисперсии, показывающий типичное отклонение значений от среднего.\n\nВ отличие от дисперсии, стандартное отклонение измеряется в тех же единицах, что и данные, поэтому его легко интерпретировать: оно показывает «обычную» величину колебаний.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Статистика"
      ]
    },
    {
      "id": 290,
      "question": "Что такое стандартная ошибка среднего?",
      "answer": "Стандартная ошибка среднего — это мера разброса оценок выборочного среднего при повторных выборках и равна $SE = \\frac{s}{\\sqrt{n}}$.\n\nОна показывает не разброс данных, а насколько точно мы знаем среднее. Чем больше выборка, тем меньше стандартная ошибка и тем стабильнее оценка среднего.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Статистика"
      ]
    },
    {
      "id": 291,
      "question": "Чем стандартное отклонение отличается от стандартной ошибки?",
      "answer": "Стандартное отклонение описывает разброс данных, а стандартная ошибка — разброс оценки среднего. Стандартное отклонение отвечает на вопрос «насколько различаются наблюдения», а стандартная ошибка — «насколько может колебаться среднее от выборки к выборке».",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Статистика"
      ]
    },
    {
      "id": 292,
      "question": "Чем отличается стандартная ошибка выборочного среднего и генеральной совокупности?",
      "answer": "Если $\\sigma$ неизвестна, стандартную ошибку считают как $SE=\\frac{s}{\\sqrt{n}}$; если $\\sigma$ известна то $SE=\\frac{\\sigma}{\\sqrt{n}}$. Разница только в том, что используется для оценки разброса, выборочная оценка или истинное стандартное отклонение генеральной совокупности.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Статистика"
      ]
    },
    {
      "id": 293,
      "question": "Чем отличается стандартное отклонение генеральной совокупности от выборочного?",
      "answer": "$\\sigma$ — истинное стандартное отклонение генеральной совокупности, а $s$ — его оценка по выборке. На практике $\\sigma$ почти всегда неизвестна, поэтому используют $s$, понимая, что это приближённая оценка, зависящая от конкретной выборки.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Статистика"
      ]
    },
    {
      "id": 294,
      "question": "Что такое IQR и зачем он нужен?",
      "answer": "IQR (межквартильный размах) — это разница между 75-м и 25-м перцентилями: ($IQR = Q_3 - Q_1$). IQR измеряет разброс центральных 50% данных и устойчив к выбросам, поэтому часто используется вместо размаха и стандартного отклонения при «тяжёлых хвостах».",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Статистика"
      ]
    },
    {
      "id": 295,
      "question": "Что такое CUPED?",
      "answer": "CUPED (Controlled Using Pre-Experiment Data) — это метод снижения дисперсии в A/B-тестах, который использует предпериодные данные как ковариату для корректировки метрики, повышая мощность теста без смещения эффекта.\n\nМетод вычитает из экспериментальной метрики часть, объясняемую прошлым поведением пользователей, уменьшая шум и позволяя быстрее обнаруживать эффект.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "A/B тесты"
      ]
    },
    {
      "id": 296,
      "question": "Как проводить CUPED на практике?",
      "answer": "1. Выбераем предпериодную ковариату $X$, коррелирующую с метрикой $Y$.\n2. Оцениваем коэффициент:  \n    $$\\theta = \\frac{\\mathrm{Cov}(Y, X)}{\\mathrm{Var}(X)}$$  \n\n3. Строим скорректированную метрику:  \n    $$Y_{\\text{adj}} = Y - \\theta (X - \\bar{X})$$  \n\n4. Запускаем обычный t-тест (или другой) на $Y_{\\text{adj}}$.\n    \nКоррекция сохраняет разницу средних между группами, но уменьшает дисперсию метрики, что повышает статистическую мощность и снижает минимально обнаружимый эффект.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "A/B тесты"
      ]
    },
    {
      "id": 297,
      "question": "Что такое ковариата?",
      "answer": "Ковариата — это дополнительная переменная, связанная с целевой метрикой и измеренная до воздействия (или не зависящая от него), которую используют для контроля или уменьшения дисперсии при анализе (например, в CUPED/ANCOVA).\n\nКовариата помогает объяснить часть естественного разброса между объектами (пользователями), чтобы эффект эксперимента было легче обнаружить.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Статистика",
        "A/B тесты"
      ]
    },
    {
      "id": 298,
      "question": "Что такое ANCOVA?",
      "answer": "ANCOVA (Analysis of Covariance)— это метод анализа A/B-теста, который оценивает эффект группы, одновременно учитывая ковариаты (например, предпериодные метрики), снижая дисперсию и повышая мощность; часто реализуется регрессией:  \n\n$$Y = \\beta_0 + \\beta_1 \\cdot T + \\beta_2 \\cdot X + \\varepsilon$$  \n\n- $Y$ — метрика в эксперименте (например, выручка в период теста),\n- $T$ — индикатор группы (A=0, B=1),\n- $X$ — ковариата (например, выручка в предпериоде),\n- $\\beta_1$ — ​искомый эффект теста.\n\nANCOVA “выравнивает” пользователей по исходным различиям через ковариаты, чтобы эффект было легче обнаружить.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "A/B тесты"
      ]
    },
    {
      "id": 299,
      "question": "Зачем нужна стратификация при рандомизации?",
      "answer": "Стратификация нужна, чтобы сбалансировать группы A и B по важным признакам (странам, платформам, уровню активности), уменьшая риск перекоса и повышая точность оценки эффекта за счёт снижения дисперсии.\n\nМы сначала делим пользователей на однородные слои и рандомизируем внутри каждого слоя, чтобы в каждой группе было сравнимое число пользователей каждого типа.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "A/B тесты"
      ]
    },
    {
      "id": 300,
      "question": "Когда стратификация не нужна или может быть опасна?",
      "answer": "Стратификация не нужна, если выборка большая и простая рандомизация уже обеспечивает баланс; она может быть опасна при слишком мелких или многочисленных стратах, когда в некоторых слоях мало наблюдений, что усложняет рандомизацию и может увеличить шум или привести к ошибкам реализации.\n\nЧрезмерная стратификация делает эксперимент сложным, увеличивает риск технических багов и может ухудшить качество теста, если внутри страт недостаточно данных.\n\nЭвристики:\n- Не стратифицируй по десяткам признаков\n- Не стратифицируй по признакам, которые могут измениться из-за эксперимента\n- Не стратифицируй при маленьком трафике\n- Стратифицируй по 1–3 самым сильным факторам\n- Стратифицируй по признакам, известным до эксперимента\n- Если сомневаешься — лучше CUPED / ANCOVA на анализе",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "A/B тесты"
      ]
    },
    {
      "id": 301,
      "question": "Что такое switch-over bias и как его избежать?",
      "answer": "Switch-over bias — это смещение в A/B-тесте, когда один пользователь попадает и в A, и в B, из-за чего эффекты смешиваются и размываются (пользователь перелогинился, очистил cookies, сменил браузер, переустановил приложение, использует два устройства); избежать можно жёсткой “липкой” привязкой варианта к user_id (или другому стабильному идентификатору, например устойчивый hash-ключ) и хранением assignment так, чтобы он не менялся между сессиями и устройствами; либо исключать/отмечать пользователей, которые переключились, если это неизбежно.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "A/B тесты"
      ]
    },
    {
      "id": 302,
      "question": "Что помогает контролировать сезонность в эксперименте?",
      "answer": "Сезонность контролируют дизайном и балансировкой по времени: используют switchback (переключение A/B по временным слотам), балансируют распределение по дням недели/часам (стратификация по календарю), и анализируют метрики с учётом временных факторов.\n\nИдея в том, чтобы оба варианта прошли через одинаковые “режимы времени” (пики, выходные, праздники), иначе разница может быть вызвана календарём, а не эффектом фичи.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "A/B тесты"
      ]
    },
    {
      "id": 303,
      "question": "Что такое SRM?",
      "answer": "SRM (Sample Ratio Mismatch) — это ситуация, когда фактические доли пользователей в вариантах не совпадают с планом. По плану: 50% / 50%, по факту: 53% / 47%. Группы могут перестать быть сопоставимыми, эффект эксперимента/теста может быть искажён.\n    \nSRM — это  симптом проблемы.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "A/B тесты"
      ]
    },
    {
      "id": 304,
      "question": "Как проверить SRM до запуска основного анализа?",
      "answer": "Сравнивают фактическое распределение пользователей по вариантам с плановым и проводят χ²-тест; при маленьком p-value (часто < 0.01) считают, что есть SRM, и эксперимент признают некорректным.\n\nSRM проверяют до анализа метрик, потому что перекос трафика обычно указывает на баг рандомизации или трекинга и делает результаты эксперимента недостоверными.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "A/B тесты"
      ]
    },
    {
      "id": 305,
      "question": "В чём преимущества CUPED и какие данные нужны?",
      "answer": "CUPED снижает дисперсию метрики и повышает мощность теста, используя предпериодные данные (ковариаты), коррелирующие с метрикой эксперимента.\n\nНужна метрика, измеренная до эксперимента и не зависящая от лечения (например, выручка пользователя в предпериоде). CUPED вычитает объяснимую часть шума, не смещая эффект.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "A/B тесты"
      ]
    },
    {
      "id": 306,
      "question": "Когда выбирать стратифицированную рандомизацию?",
      "answer": "Стратифицированную рандомизацию используют, когда известно, что некоторые признаки (страна, платформа, активность) сильно влияют на метрику и важно сбалансировать группы по ним уже на этапе распределения трафика.\n\nСтратификация уменьшает риск перекоса и снижает дисперсию оценки эффекта, но требует достаточного трафика и аккуратной реализации, иначе может усложнить эксперимент.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "A/B тесты"
      ]
    },
    {
      "id": 307,
      "question": "Что показывает нотация Big-O и зачем она нужна?",
      "answer": "Big-O показывает, как растут время или память алгоритма при увеличении размера входных данных, и позволяет сравнивать алгоритмы по масштабируемости. Big-O не измеряет секунды или миллисекунды, а описывает порядок роста. Это нужно, чтобы понимать, как алгоритм будет вести себя на больших данных и выбирать решения, которые не «сломаются» при росте нагрузки.",
      "tags": [],
      "level": "lvl_1",
      "topic": [
        "Алгоритмы"
      ]
    },
    {
      "id": 308,
      "question": "Почему в Big-O отбрасывают константы и младшие члены?",
      "answer": "Потому что при больших размерах входных данных вклад констант и младших членов становится несущественным. Big-O фокусируется на поведении алгоритма при росте $N$. Например, $O(3N + 10)$ и $O(N)$ ведут себя одинаково при больших $N$, поэтому их считают эквивалентными по сложности.",
      "tags": [],
      "level": "lvl_1",
      "topic": [
        "Алгоритмы"
      ]
    },
    {
      "id": 309,
      "question": "Почему обычно анализируют худший случай, а не средний?",
      "answer": "Худший случай даёт гарантированную верхнюю границу времени работы алгоритма и не зависит от входных данных. Средний случай часто сложнее оценить и он зависит от характера данных. Худший случай даёт надёжную границу и позволяет понять, насколько алгоритм безопасен в самых неблагоприятных условиях.",
      "tags": [],
      "level": "lvl_1",
      "topic": [
        "Алгоритмы"
      ]
    },
    {
      "id": 310,
      "question": "Чем алгоритмическая сложность Big-O отличается от реального времени выполнения?",
      "answer": "Алгоритмическая сложность описывает асимптотический рост ресурсов, а реальное время выполнения зависит от констант, реализации, языка и железа. Big-O — это абстрактная модель для сравнения алгоритмов, а реальное время — практический результат конкретного кода. Алгоритм с лучшей Big-O может быть медленнее на малых данных из-за больших констант.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Алгоритмы"
      ]
    },
    {
      "id": 311,
      "question": "Как определить временную сложность кода с одним циклом?",
      "answer": "Если цикл выполняется $N$ раз и внутри него операции за $O(1)$, то временная сложность равна $O(N)$. Каждая итерация добавляет постоянное время, поэтому общее время растёт линейно с размером входных данных.",
      "tags": [],
      "level": "lvl_1",
      "topic": [
        "Алгоритмы"
      ]
    },
    {
      "id": 312,
      "question": "Как определить сложность вложенных циклов?",
      "answer": "Сложность равна произведению числа итераций циклов, например два вложенных цикла по $N$ дают $O(N^2)$. Внутренний цикл полностью выполняется для каждой итерации внешнего, поэтому общее число операций растёт квадратично.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Алгоритмы"
      ]
    },
    {
      "id": 313,
      "question": "Какова сложность кода с двумя независимыми циклами?",
      "answer": "Если два цикла идут последовательно и каждый выполняется $N$ раз, общая сложность равна $O(N + N) = O(N)$. Последовательные циклы складывают количество операций, а не перемножают их, поэтому доминирует линейный рост.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Алгоритмы"
      ]
    },
    {
      "id": 314,
      "question": "В чём разница между $O(N)$ и $O(N + M)$?",
      "answer": "$O(N)$ описывает работу с одним входом, а $O(N + M)$ — с двумя независимыми входами разных размеров. Важно явно указывать все независимые размеры входных данных; иначе можно ошибочно упростить сложность и недооценить рост времени.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Алгоритмы"
      ]
    },
    {
      "id": 315,
      "question": "Когда $O(N + M)$ можно упростить до $O(N)$?",
      "answer": "Когда $M$ ограничено константой или всегда пропорционально $N$. Если размеры входов растут вместе или один из них мал по сравнению с другим, асимптотически доминирует один член, и сложность можно упростить.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Алгоритмы"
      ]
    },
    {
      "id": 316,
      "question": "Почему бинарный поиск имеет сложность $O(\\log N)$?",
      "answer": "Потому что на каждом шаге бинарный поиск делит область поиска пополам. Число шагов равно количеству раз, которое можно делить $N$ на 2, пока не останется один элемент — это и есть $\\log_2 N$. Поэтому рост времени очень медленный даже при больших $N$.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Алгоритмы"
      ]
    },
    {
      "id": 317,
      "question": "Почему большинство эффективных сортировок имеют сложность $O(N \\log N)$?",
      "answer": "Потому что сортировки сравнениями требуют $O(\\log N)$ уровней разбиения и выполняют $O(N)$ операций на каждом уровне. Алгоритмы вроде merge sort и quicksort рекурсивно делят массив, а затем обрабатывают все элементы на каждом уровне, что даёт итоговую сложность $N \\log N$.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Алгоритмы"
      ]
    },
    {
      "id": 318,
      "question": "Почему $O(N^2)$ быстро становится неприемлемым на практике?",
      "answer": "Потому что время работы растёт квадратично и быстро выходит за разумные пределы при увеличении $N$. При росте данных в 10 раз время работы увеличивается в 100 раз. Поэтому алгоритмы с $O(N^2)$ приемлемы только для маленьких входов и плохо масштабируются.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Алгоритмы"
      ]
    },
    {
      "id": 319,
      "question": "Как мемоизация или DP меняют Big-O рекурсивного алгоритма?",
      "answer": "Они устраняют повторные вычисления, снижая сложность с экспоненциальной до линейной или полиномиальной. Каждая подзадача решается один раз и сохраняется, поэтому общее число операций пропорционально числу различных состояний.",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "Алгоритмы"
      ]
    },
    {
      "id": 320,
      "question": "Как оценить сложность рекурсивной функции по дереву вызовов?",
      "answer": "Нужно оценить количество узлов в дереве вызовов и стоимость работы в каждом узле. Общая сложность равна произведению глубины дерева, ветвления и стоимости операций на уровне, что позволяет понять порядок роста алгоритма.",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "Алгоритмы"
      ]
    },
    {
      "id": 321,
      "question": "Почему алгоритм с лучшей Big-O может быть медленнее на малых данных?",
      "answer": "Из-за больших констант, накладных расходов и более сложной реализации. Big-O описывает поведение при больших $N$, но на маленьких входах простые алгоритмы с худшей асимптотикой часто работают быстрее.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Алгоритмы"
      ]
    },
    {
      "id": 322,
      "question": "Как оценить, «влезет» ли алгоритм по времени при $N = 10^6$?",
      "answer": "Нужно прикинуть число операций и сравнить его с допустимым бюджетом операций в секунду. Грубо считают, что $10^7-10^8$ простых операций в секунду — разумный предел; если оценка сильно больше, алгоритм не подойдёт.",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "Алгоритмы"
      ]
    },
    {
      "id": 323,
      "question": "Когда оптимизация по Big-O не имеет смысла?",
      "answer": "Когда входные данные малы, узкое место в I/O или сети, либо код выполняется редко.\nВ таких случаях выигрыш от улучшения асимптотики меньше, чем затраты на усложнение кода и поддержку.",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "Алгоритмы"
      ]
    },
    {
      "id": 324,
      "question": "Как работает merge sort и почему у него сложность $O(N \\log N)$?",
      "answer": "Merge sort рекурсивно делит массив пополам, сортирует части и сливает их. Всего $\\log N$ уровней разбиения и $O(N)$ работы на каждом уровне. На каждом уровне рекурсии элементы просто объединяются за линейное время, а глубина рекурсии равна числу делений массива пополам, то есть $\\log N$.",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "Алгоритмы"
      ]
    },
    {
      "id": 325,
      "question": "Как работает quicksort и в чём его отличие от merge sort?",
      "answer": "Quicksort выбирает опорный элемент, разбивает массив на меньшие и большие элементы и рекурсивно сортирует части; в среднем имеет $O(N \\log N)$. В отличие от merge sort, quicksort работает in-place и обычно быстрее на практике, но в худшем случае может деградировать до $O(N^2)$.",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "Алгоритмы"
      ]
    },
    {
      "id": 326,
      "question": "Когда имеет смысл использовать сортировку с двумя указателями?",
      "answer": "Когда данные уже частично отсортированы или задача сводится к линейному слиянию/поиску пар. Два указателя позволяют решать задачи за $O(N)$, избегая вложенных циклов, например при слиянии отсортированных массивов или поиске пары с заданной суммой.",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "Алгоритмы"
      ]
    },
    {
      "id": 327,
      "question": "Что такое рекурсия и какие у неё плюсы и минусы?",
      "answer": "Рекурсия — это вызов функцией самой себя для разбиения задачи на подзадачи. Плюсы — компактный и наглядный код; минусы — накладные расходы, риск переполнения стека и сложность отладки.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Алгоритмы"
      ]
    },
    {
      "id": 328,
      "question": "Когда рекурсия опасна в production-коде?",
      "answer": "При глубокой рекурсии, большом входе или отсутствии контроля глубины вызовов. Это может привести к stack overflow и нестабильной работе сервиса; в production часто предпочитают итеративные решения или явный стек.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Алгоритмы"
      ]
    },
    {
      "id": 329,
      "question": "В чём отличие динамического программирования от жадных алгоритмов?",
      "answer": "Динамическое программирование перебирает и сохраняет решения подзадач, а жадные алгоритмы делают локально оптимальный выбор на каждом шаге.\n\nDP гарантирует глобальный оптимум ценой памяти и времени, а жадные алгоритмы быстрее и проще, но работают только для задач с особыми свойствами.",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "Алгоритмы"
      ]
    },
    {
      "id": 330,
      "question": "В чём разница между memoization и tabulation?",
      "answer": "Memoization — это кеширование результатов рекурсивных вызовов, а tabulation — итеративное заполнение таблицы снизу вверх. Оба подхода уменьшают число вычислений; memoization проще внедрить в рекурсивный код, tabulation обычно быстрее и безопаснее для production.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Алгоритмы"
      ]
    },
    {
      "id": 331,
      "question": "Что такое связный список и чем он отличается от массива?",
      "answer": "Связный список — структура данных, где элементы хранят ссылки друг на друга, в отличие от массива с непрерывным хранением в памяти. В массиве доступ по индексу быстрый, но вставки и удаления дорогие. В связном списке вставки и удаления дешевле, но доступ к элементу требует последовательного обхода.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Python",
        "Алгоритмы"
      ]
    },
    {
      "id": 332,
      "question": "Почему связные списки редко используются в ML/DS-коде на Python?",
      "answer": "Потому что массивы и numpy-структуры эффективнее по памяти и скорости доступа, а связные списки плохо векторизуются. В ML/DS важны плотные данные, быстрый доступ и операции над массивами, чего связные списки не обеспечивают в Python.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "ML",
        "Python",
        "Алгоритмы"
      ]
    },
    {
      "id": 333,
      "question": "Что такое алгоритм с двумя указателями и где он применяется?",
      "answer": "Это алгоритмический приём, при котором два указателя двигаются по данным для решения задачи за линейное время. Часто используется для работы с отсортированными массивами, поиска пар, слияния массивов и устранения вложенных циклов.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Алгоритмы"
      ]
    },
    {
      "id": 334,
      "question": "Как определить, можно ли оптимизировать алгоритм без вложенных циклов?",
      "answer": "Если один и тот же массив просматривается многократно, часто можно заменить вложенные циклы на линейный проход с указателями или вспомогательной структурой.\n\nПовторные проходы по данным — сигнал к оптимизации; использование хеш-таблиц, сортировки или двух указателей часто снижает сложность с $O(N^2)$ до $O(N)$ или $O(N \\log N)$.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Алгоритмы"
      ]
    },
    {
      "id": 335,
      "question": "Что такое ООП и зачем оно нужно в алгоритмическом коде?",
      "answer": "ООП — это подход к программированию, при котором логика и данные объединяются в объекты, что упрощает структуру, повторное использование и расширение алгоритмов.\n \nВ алгоритмическом коде ООП помогает управлять сложностью: разделять ответственность, инкапсулировать детали реализации и переиспользовать компоненты в больших пайплайнах.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Алгоритмы"
      ]
    },
    {
      "id": 336,
      "question": "В чём суть инкапсуляции и какую проблему она решает?",
      "answer": "Инкапсуляция скрывает внутреннюю реализацию объекта и предоставляет доступ к данным через публичный интерфейс. Это защищает код от некорректного использования, уменьшает связанность компонентов и позволяет менять реализацию без влияния на остальную систему.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Алгоритмы"
      ]
    },
    {
      "id": 337,
      "question": "Что такое наследование и когда его не стоит использовать?",
      "answer": "Наследование это создание нового классы на основе существующего. Не стоит использовать наследование, если нет отношения «is-a». Чрезмерное наследование усложняет код и делает его хрупким; часто предпочтительнее композиция, когда объекты содержат другие объекты.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Алгоритмы"
      ]
    },
    {
      "id": 338,
      "question": "Что такое полиморфизм и как он помогает писать расширяемый код?",
      "answer": "Полиморфизм это использование единого интерфейса для объектов разных типов с разной реализацией. Он даёт возможность добавлять новые алгоритмы или модели без изменения существующего кода, что особенно важно в расширяемых системах.",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "Алгоритмы"
      ]
    },
    {
      "id": 339,
      "question": "Что такое абстракция и как она используется в проектировании алгоритмов?",
      "answer": "Абстракция выделяет только существенные свойства объекта, скрывая детали реализации. В алгоритмах абстракция позволяет проектировать решения на уровне идей и интерфейсов (например, «оптимизатор», «модель», «метрика»), не привязываясь к конкретной реализации.",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "Алгоритмы"
      ]
    },
    {
      "id": 340,
      "question": "Что такое асинхронность и когда она полезна в задачах ML/DS?",
      "answer": "Асинхронность — это способ выполнять операции без блокировки основного потока, позволяя ждать I/O и выполнять другие задачи. В ML/DS асинхронность полезна при работе с API, базами данных, файловыми системами и онлайн-инференсом, где узким местом является ожидание ответа, а не вычисления.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "ML",
        "Алгоритмы"
      ]
    },
    {
      "id": 341,
      "question": "Что такое граф и где графовые алгоритмы применяются в ML?",
      "answer": "Граф — это структура данных из вершин и рёбер, описывающая связи между объектами. В ML графы используют в рекомендациях, социальных сетях,  анализе связей, графовых нейросетях (GNN) и задачах маршрутизации.",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "ML",
        "Алгоритмы"
      ]
    },
    {
      "id": 342,
      "question": "Что такое хеш-таблица и почему доступ в ней амортизирован $O(1)?$",
      "answer": "Хеш-таблица хранит данные по хеш-ключам и обеспечивает быстрый доступ к элементам. При хорошем хешировании операции вставки и поиска в среднем выполняются за константное время, а редкие перестроения таблицы учитываются в амортизированной оценке.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Алгоритмы"
      ]
    },
    {
      "id": 343,
      "question": "Почему NumPy быстрее обычных Python-циклов?",
      "answer": "NumPy выполняет операции в скомпилированном C-коде и использует векторизацию. Это снижает накладные расходы Python-интерпретатора и позволяет эффективно использовать CPU и кэш, что особенно важно при работе с большими массивами данных.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Python",
        "Алгоритмы"
      ]
    },
    {
      "id": 344,
      "question": "Почему наивная рекурсия Фибоначчи имеет экспоненциальную сложность?",
      "answer": "Потому что одна и та же подзадача вычисляется многократно, и число вызовов растёт экспоненциально. Каждый вызов порождает два новых, образуя дерево вызовов, размер которого растёт примерно как $2^N$, из-за чего алгоритм быстро становится неэффективным.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "Алгоритмы"
      ]
    },
    {
      "id": 345,
      "question": "Как мыслить порядками величин, чтобы понять, будет ли алгоритм работать достаточно быстро?",
      "answer": "Нужно оценить порядок роста алгоритма (Big-O), прикинуть число операций при заданном $N$ и сравнить его с разумным бюджетом операций во времени выполнения.\n\nСначала определяем асимптотическую сложность (например, $O(N)$, $O(N\\log N)$, $O(N^2)$), затем подставляем реальный масштаб данных (например, $N \\approx 10^6$). Если получается десятки миллионов операций — алгоритм обычно допустим; если миллиарды и больше — скорее всего нет. Это позволяет быстро отсечь неподходящие решения ещё до написания кода.",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "Алгоритмы",
        "Мышление"
      ]
    },
    {
      "id": 346,
      "question": "Чем пузырьковая сортировка отличается от quicksort и merge sort?",
      "answer": "Пузырьковая сортировка имеет квадратичную сложность $O(N^2)$ и используется в основном в учебных целях, тогда как quicksort и merge sort имеют сложность $O(N \\log N)$ и применяются на практике.\n\nBubble sort последовательно меняет соседние элементы и плохо масштабируется. Quicksort и merge sort используют стратегию «разделяй и властвуй», работают значительно быстрее на больших данных и являются базой для промышленных реализаций сортировки.",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "Алгоритмы"
      ]
    },
    {
      "id": 347,
      "question": "Сколько фур нужно, чтобы перевезти гору Фудзи? (метод Ферми)",
      "answer": "Метод Ферми, или \"решение на салфетке\". Здесь точность не важна — важно попасть в масштаб и уметь объяснить допущения. \n\nОбъём горы (очень грубо считаем как конус). Высота Фудзи $H \\approx 4 \\ \\text{км}$. Радиус основания возьмём $R \\approx 20 \\ \\text{км}$ (значит диаметр около $40 \\ \\text{км}$). Тогда:  \n    $$  \n    V \\approx \\frac{1}{3} \\cdot \\pi \\cdot R^2 \\cdot H  \n    \\approx \\frac{1}{3}\\cdot 3 \\cdot (20^2)\\cdot 4  \n    \\approx 1600 \\ \\text{км}^3  \n    $$\n    \nПереводим в кубометры. $1 \\ \\text{км}^3 = 10^9 \\ \\text{м}^3$, значит: \n$$V \\approx 1.6 \\times 10^{12} \\ \\text{м}^3$$\n\nМасса горы через плотность породы. Для “камня” берём $\\rho \\approx 3 \\ \\text{т/м}^3$, тогда:  \n    $$  \n    M \\approx \\rho \\cdot V \\approx 3 \\cdot 1.6 \\times 10^{12} \\approx 4.8 \\times 10^{12} \\ \\text{т}  \n    $$\n    \nКоличество фур. Пусть одна фура перевозит $\\approx 10 \\ \\text{т}$, тогда:  \n    $$  \n    \\frac{M}{10}  \n    \\approx \\frac{4.8 \\times 10^{12}}{10}  \n    \\approx 4.8 \\times 10^{11}  \n    $$\n    \n\nИтого, чтобы перевезти гору Фудзи понадобится порядка $10^{11}$–$10^{12}$ фур (то есть сотни миллиардов).",
      "tags": [],
      "level": "lvl_3",
      "topic": [
        "Мышление"
      ]
    },
    {
      "id": 348,
      "question": "Почему object detection подходит лучше, чем classification, для задач анализа экрана CV-моделей?",
      "answer": "Потому что object detection находит и локализует несколько объектов на изображении, а classification выдаёт только один общий класс для всего кадра. В анализе экрана важно понимать где именно находится каждый элемент интерфейса, а не просто «что изображено». Classification теряет пространственную информацию, тогда как detection сохраняет координаты и размеры объектов.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "ML"
      ]
    },
    {
      "id": 349,
      "question": "Что означает mAP@50–95 и почему одной accuracy недостаточно для CV-моделей?",
      "answer": "mAP@50–95 — это средняя точность детекции при разных IoU-порогах от 0.50 до 0.95, усреднённая по классам. Accuracy не учитывает локализацию объектов и баланс классов. В CV важно не только угадать класс, но и насколько точно найден объект, поэтому используют mAP, который учитывает и классификацию, и качество bounding box.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "ML"
      ]
    },
    {
      "id": 350,
      "question": "Как влияет размер input resolution (например, 640 vs 768) на качество и скорость в задачах CV?",
      "answer": "Большее разрешение улучшает детекцию мелких объектов, но увеличивает время инференса и потребление памяти. При увеличении resolution модель видит больше деталей, но число вычислений растёт квадратично. В real-time задачах всегда ищут компромисс между качеством и FPS.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "ML"
      ]
    },
    {
      "id": 351,
      "question": "Почему важно выбирать confidence threshold и IoU threshold, и как они влияют на результат в задачах CV?",
      "answer": "Эти пороги управляют балансом между ложными срабатываниями и пропусками объектов. Confidence threshold отсекает неуверенные предсказания. IoU threshold определяет, считать ли предсказание корректным. Слишком низкие пороги дают много false positives, слишком высокие — пропущенные объекты (false negative).",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "ML"
      ]
    },
    {
      "id": 352,
      "question": "Какие типичные источники ошибок CV-модели в real-time инференсе?",
      "answer": "Шум, изменение освещения, перекрытие объектов, задержки инференса и несоответствие train/test условий. Модель часто обучается на статичных данных, а в реальном времени сталкивается с motion blur, масштабированием, артефактами видео и latency, что снижает стабильность предсказаний.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "ML"
      ]
    },
    {
      "id": 353,
      "question": "Почему Monte-Carlo симуляции применяются вместо точного перебора состояний в сложных вероятностных задачах?",
      "answer": "Потому что точный перебор имеет экспоненциальную сложность и быстро становится вычислительно невозможным. Monte-Carlo приближает решение за счёт случайного семплирования, позволяя получить хорошую оценку за разумное время, особенно в задачах с огромным пространством состояний (например, покер или Го).",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "ML",
        "Алгоритмы"
      ]
    },
    {
      "id": 354,
      "question": "Какие допущения делает Monte-Carlo подход и где он может давать систематическую ошибку?",
      "answer": "Monte-Carlo предполагает репрезентативное случайное семплирование и независимость испытаний. Если распределение семплов смещено или число симуляций недостаточно, оценки могут быть систематически неверными, особенно в редких или крайних сценариях.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "ML"
      ]
    },
    {
      "id": 355,
      "question": "Почему CatBoost хорошо работает с категориальными признаками без one-hot encoding?",
      "answer": "CatBoost хорошо работает с категориальными признаками, потому что вместо one-hot он кодирует категории через статистику таргета, но делает это упорядоченно, без утечки информации, что снижает размерность и улучшает обобщение.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "ML"
      ]
    },
    {
      "id": 356,
      "question": "Зачем нужна регуляризация в градиентном бустинге?",
      "answer": "Регуляризация ограничивает сложность модели и снижает переобучение. Параметр регуляризации штрафует большие веса в листьях деревьев, делая модель более устойчивой к шуму и выбросам в данных.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "ML"
      ]
    },
    {
      "id": 357,
      "question": "Почему MAPE может быть плохой метрикой при наличии выбросов?",
      "answer": "MAPE, Mean Absolute Percentage Error $= \\frac{1}{n}\\cdot \\sum_{i=1}^{n}\\left|\\frac{y_i - \\hat{y}_i}{y_i}\\right| \\cdot$ 100%, показывает на сколько процентов в среднем ошибается модель относительно истинных значений. Сильно искажается при выбросах, малых или нулевых значениях. Даже небольшая ошибка при малом фактическом значении приводит к огромному проценту, из-за чего метрика становится нестабильной и вводящей в заблуждение.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "ML"
      ]
    },
    {
      "id": 358,
      "question": "Зачем смотреть SHAP-значения, если модель уже хорошо предсказывает?",
      "answer": "Чтобы понять, какие признаки и как влияют на предсказания модели. SHAP помогает выявить утечки, смещения и неочевидные зависимости, а также повышает доверие к модели со стороны бизнеса и команды.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "ML"
      ]
    },
    {
      "id": 359,
      "question": "Почему простая baseline-модель важна перед сложным тюнингом?",
      "answer": "Baseline задаёт точку отсчёта для оценки реального выигрыша от усложнения модели. Без baseline невозможно понять, улучшает ли сложная модель качество или просто добавляет шум и сложность без значимого прироста.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "ML"
      ]
    },
    {
      "id": 360,
      "question": "Почему матричная факторизация хорошо работает на разреженных данных?",
      "answer": "Потому что она обучается только на известных значениях и не требует заполнения пропусков. В рекомендательных системах матрица «пользователь–объект» почти всегда разрежена. Матричная факторизация извлекает скрытую структуру из наблюдаемых взаимодействий, игнорируя отсутствующие значения, что делает её устойчивой и эффективной.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "ML"
      ]
    },
    {
      "id": 361,
      "question": "Что означают латентные факторы в матричной факторизации — математически и интуитивно?",
      "answer": "Математически — это скрытые векторы меньшей размерности, произведение которых приближает исходную матрицу; интуитивно — это неявные предпочтения и свойства. Латентные факторы можно интерпретировать как скрытые «темы» или «вкусы»: например, в кино — жанровые предпочтения пользователя и характеристики фильмов.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "ML"
      ]
    },
    {
      "id": 362,
      "question": "Почему добавляют регуляризацию в matrix factorization и что будет без неё?",
      "answer": "Регуляризация предотвращает переобучение и рост весов латентных факторов. Без регуляризации модель начинает подгоняться под шум в данных, особенно при малом числе наблюдений, что ухудшает обобщающую способность.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "ML"
      ]
    },
    {
      "id": 363,
      "question": "Почему нельзя просто заменить пропуски средним и обучить регрессию в задаче матричной факторизации?",
      "answer": "Потому что пропуски не означают нулевую или среднюю оценку, а их заполнение искажает структуру данных. Заполнение средним уничтожает информацию о разреженности и приводит к смещённым оценкам, тогда как матричная факторизация изначально учитывает отсутствие наблюдений.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "ML"
      ]
    },
    {
      "id": 364,
      "question": "Почему сверточные сети лучше подходят для изображений, чем полносвязные?",
      "answer": "Потому что свёртки учитывают локальную структуру изображения и используют разделяемые веса. CNN эффективно извлекают пространственные паттерны (края, формы) и требуют гораздо меньше параметров, чем полносвязные сети, что улучшает обобщение.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "ML"
      ]
    },
    {
      "id": 365,
      "question": "Почему softmax-вероятности не всегда означают уверенность модели?",
      "answer": "Потому что softmax всегда нормализует выходы до 1, даже если модель плохо обучена или видит незнакомые данные. Высокая softmax-вероятность может быть следствием относительного сравнения классов, а не истинной уверенности; модель может быть переуверенной на out-of-distribution данных.",
      "tags": [],
      "level": "lvl_2",
      "topic": [
        "ML"
      ]
    }
  ]
}