

Что такое Attention?

---

Attention (Внимание) — это механизм в трансформерах, который позволяет каждому слову учитывать контекст других слов для уточнения своего смысла.

---

#lvl_2 